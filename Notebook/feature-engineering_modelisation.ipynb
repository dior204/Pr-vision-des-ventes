{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "13897552-a077-4cfd-ab9c-a5ef2782d7c0",
    "_uuid": "c4f8c69b-de9a-4a20-8e64-5aa1c5e392e3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Feature engineering et Mod√©lisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventaires des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "95b31b5d-8abe-49ca-8bb0-beba8b1e2e78",
    "_uuid": "6a5c0f86-04a6-4fb6-9942-14a8e7121029",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-13T14:20:41.799289Z",
     "iopub.status.busy": "2026-01-13T14:20:41.798842Z",
     "iopub.status.idle": "2026-01-13T14:20:41.812544Z",
     "shell.execute_reply": "2026-01-13T14:20:41.811291Z",
     "shell.execute_reply.started": "2026-01-13T14:20:41.799253Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers disponibles :\n",
      "test.csv.7z\n",
      "stores.csv.7z\n",
      "items.csv.7z\n",
      "holidays_events.csv.7z\n",
      "transactions.csv.7z\n",
      "train.csv.7z\n",
      "oil.csv.7z\n",
      "sample_submission.csv.7z\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/favorita-grocery-sales-forecasting\"\n",
    "\n",
    "print(\"Fichiers disponibles :\")\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "150f945a-01e3-492f-bd4f-22cab30465fa",
    "_uuid": "b1f0c07b-4e19-4343-9f92-a5e1caa0a883",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Code de d√©compression des fichiers\n",
    "!! n'ex√©cuter qu'une seule fois !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "1992fdf2-3bb5-4025-82ff-5f8f9ac59c6e",
    "_uuid": "e534db69-81cf-4012-896b-83fe36c8c19c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-13T14:20:54.406837Z",
     "iopub.status.busy": "2026-01-13T14:20:54.406325Z",
     "iopub.status.idle": "2026-01-13T14:22:03.686107Z",
     "shell.execute_reply": "2026-01-13T14:22:03.684691Z",
     "shell.execute_reply.started": "2026-01-13T14:20:54.406802Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©compression de test.csv.7z ...\n",
      "D√©compression de stores.csv.7z ...\n",
      "D√©compression de items.csv.7z ...\n",
      "D√©compression de holidays_events.csv.7z ...\n",
      "D√©compression de transactions.csv.7z ...\n",
      "D√©compression de train.csv.7z ...\n",
      "D√©compression de oil.csv.7z ...\n",
      "D√©compression de sample_submission.csv.7z ...\n",
      "\n",
      "Fichiers d√©compress√©s :\n",
      "['oil.csv', 'test.csv', 'holidays_events.csv', 'train.csv', 'transactions.csv', 'items.csv', 'stores.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "EXTRACT_DIR = \"/kaggle/working/favorita_data\"\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith(\".7z\"):\n",
    "        archive_path = os.path.join(DATA_DIR, file)\n",
    "        print(f\"D√©compression de {file} ...\")\n",
    "        subprocess.run(\n",
    "            [\"7z\", \"x\", archive_path, f\"-o{EXTRACT_DIR}\", \"-y\"],\n",
    "            stdout=subprocess.DEVNULL\n",
    "        )\n",
    "\n",
    "print(\"\\nFichiers d√©compress√©s :\")\n",
    "print(os.listdir(EXTRACT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e1e6aa09-278e-49e0-9ebe-b2a638c60d99",
    "_uuid": "568a5678-01a8-48f9-8ab1-13748ee49385",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Importations et configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "4127b3a5-f0c2-4dd4-bd30-9a8297bfcf98",
    "_uuid": "5877e35b-e8be-45a2-b1e1-a3d2b94fc25b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-13T14:22:03.740672Z",
     "iopub.status.busy": "2026-01-13T14:22:03.740269Z",
     "iopub.status.idle": "2026-01-13T14:22:03.759921Z",
     "shell.execute_reply": "2026-01-13T14:22:03.758469Z",
     "shell.execute_reply.started": "2026-01-13T14:22:03.740635Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0d6f691f-9713-4990-8c4f-99bca418db0c",
    "_uuid": "827e2ada-d552-48dc-a4cc-2e9d2fc25062",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Chargement des donn√©es de la base train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "dc64b898-be7a-4f0b-95b2-ed7bff0f968d",
    "_uuid": "629ca447-5d95-4780-9cce-b369e6917644",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-13T14:22:03.761733Z",
     "iopub.status.busy": "2026-01-13T14:22:03.761405Z",
     "iopub.status.idle": "2026-01-13T14:23:49.659972Z",
     "shell.execute_reply": "2026-01-13T14:23:49.659068Z",
     "shell.execute_reply.started": "2026-01-13T14:22:03.761690Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Train\n",
      "(125497040, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>103665</td>\n",
       "      <td>7.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105574</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105575</td>\n",
       "      <td>2.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108079</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108701</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_nbr  item_nbr  unit_sales onpromotion\n",
       "0 2013-01-01         25    103665        7.00         NaN\n",
       "1 2013-01-01         25    105574        1.00         NaN\n",
       "2 2013-01-01         25    105575        2.00         NaN\n",
       "3 2013-01-01         25    108079        1.00         NaN\n",
       "4 2013-01-01         25    108701        1.00         NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\n",
    "    f\"{EXTRACT_DIR}/train.csv\",\n",
    "    usecols=[\"date\", \"store_nbr\", \"item_nbr\", \"unit_sales\", \"onpromotion\"],\n",
    "    parse_dates=[\"date\"],\n",
    "    low_memory=False\n",
    ")\n",
    "print(\"Base Train\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d8e597d2-dbf9-477d-8ab9-83ca628e0e90",
    "_uuid": "ab7ed583-b45c-4862-8216-d83b87c02ea1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f726b304-89aa-4e44-a82a-c4f93592e0de",
    "_uuid": "55d85d6d-b199-4b3d-b1a2-fef827a3a4f5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 1. Mise en place du Pipeline de Feature Engineering  \n",
    "### Logique globale, pr√©vention du data leakage, *feature gap* et gestion des nouvelles donn√©es\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Pourquoi un pipeline de Feature Engineering ?\n",
    "\n",
    "Dans un projet de **pr√©vision des ventes**, le Feature Engineering ne doit pas √™tre con√ßu\n",
    "comme une suite de transformations ponctuelles appliqu√©es une seule fois √† un jeu de donn√©es.\n",
    "Il doit au contraire √™tre pens√© comme un **processus reproductible, coh√©rent et robuste**,\n",
    "capable de fonctionner dans plusieurs contextes :\n",
    "\n",
    "- entra√Ænement du mod√®le,\n",
    "- validation temporelle,\n",
    "- test final,\n",
    "- tests unitaires,\n",
    "- pr√©diction en production (nouvelles dates jamais observ√©es).\n",
    "\n",
    "Pour r√©pondre √† ces exigences, nous avons regroup√© **l‚Äôensemble des transformations**\n",
    "dans un **pipeline unique**, structur√© selon la logique **Fit / Transform** :\n",
    "\n",
    "- **FIT** : apprentissage √† partir de l‚Äôhistorique,\n",
    "- **TRANSFORM** : application des r√®gles apprises √† de nouvelles donn√©es.\n",
    "\n",
    "Cette organisation garantit :\n",
    "- l‚Äôabsence de fuite d‚Äôinformation (*data leakage*),\n",
    "- la coh√©rence des features entre train, validation, test et production,\n",
    "- la r√©utilisabilit√© du code dans un cadre r√©el.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1.1 Deux notions distinctes : *gap de split* vs *gap de features*\n",
    "\n",
    "Dans notre approche, il est important de distinguer deux id√©es souvent confondues :\n",
    "\n",
    "1) **Gap de split temporel**  \n",
    "Il s‚Äôagit d‚Äôun intervalle (ex. 3 jours) ins√©r√© entre `train_fit` et `test` lors de l‚Äô√©valuation,\n",
    "pour simuler une vraie s√©paration temporelle.\n",
    "\n",
    "2) **Feature gap (`feature_gap_days`) : retard d‚Äôacc√®s aux informations**  \n",
    "Il s‚Äôagit d‚Äôun param√®tre du pipeline qui simule un contexte r√©aliste o√π certaines informations\n",
    "(ne serait-ce que pour des raisons de consolidation ou de disponibilit√©) ne sont pas connues imm√©diatement.\n",
    "Par exemple, avec `feature_gap_days = 3`, les features bas√©es sur les ventes et transactions\n",
    "n‚Äôutilisent pas les 3 jours les plus r√©cents disponibles.\n",
    "\n",
    "üëâ Le *gap de split* sert l‚Äô√©valuation, tandis que le *feature gap* sert la pr√©vention du leakage\n",
    "dans la construction de certaines variables temporelles.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 √âtape FIT : apprentissage √† partir de l‚Äôhistorique (`train_fit`)\n",
    "\n",
    "L‚Äô√©tape `fit()` est r√©alis√©e **une seule fois**, exclusivement √† partir des donn√©es\n",
    "historiques d‚Äôentra√Ænement (`train_fit`), c‚Äôest-√†-dire **ant√©rieures √† toute p√©riode √† pr√©dire**.\n",
    "\n",
    "#### Principe fondamental\n",
    "\n",
    "Lors du `fit()` :\n",
    "- **aucune information future n‚Äôest utilis√©e**,\n",
    "- aucune statistique n‚Äôest recalcul√©e sur les p√©riodes de validation ou de test,\n",
    "- le pipeline se contente d‚Äô**apprendre et de stocker** des objets r√©utilisables ensuite\n",
    "(s√©ries index√©es, dictionnaires de mapping, fr√©quences, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.1 Encodages statistiques appris sur l‚Äôhistorique\n",
    "\n",
    "Certaines variables cat√©gorielles sont transform√©es via des **encodages de fr√©quence** :\n",
    "\n",
    "- `store_freq` : proportion d‚Äôobservations associ√©es √† chaque magasin,\n",
    "- `item_freq` : proportion d‚Äôobservations associ√©es √† chaque produit.\n",
    "\n",
    "Ces fr√©quences sont calcul√©es **uniquement sur `train_fit`**, puis m√©moris√©es dans le pipeline.\n",
    "\n",
    "Lors du `transform()`, ces valeurs sont simplement r√©appliqu√©es par `map()`.\n",
    "Si un magasin ou un produit n‚Äôa jamais √©t√© observ√©, la fr√©quence est fix√©e √† 0.\n",
    "\n",
    "üëâ Cela emp√™che toute fuite d‚Äôinformation issue des donn√©es futures.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.2 Int√©gration des tables statiques (`items.csv`, `stores.csv`)\n",
    "\n",
    "Les tables `items` et `stores` contiennent des informations **structurelles**, ind√©pendantes du temps.\n",
    "\n",
    "#### a) Table `items.csv`\n",
    "\n",
    "√Ä partir de `items.csv`, le pipeline cr√©e :\n",
    "\n",
    "- `perishable` : indicateur de p√©rissabilit√©,\n",
    "- `family_freq_items` : fr√©quence des familles de produits,\n",
    "- `class_freq_items` : fr√©quence des classes,\n",
    "- `class_freq_in_family` : fr√©quence conditionnelle classe / famille.\n",
    "\n",
    "Ces informations sont nettoy√©es, encod√©es num√©riquement,\n",
    "puis stock√©es sous forme d‚Äôune table compacte utilis√©e par `merge()`.\n",
    "\n",
    "---\n",
    "\n",
    "#### b) Table `stores.csv`\n",
    "\n",
    "√Ä partir de `stores.csv`, le pipeline extrait :\n",
    "\n",
    "- `cluster` : cluster du magasin,\n",
    "- `type_freq`, `state_freq`, `city_freq` : fr√©quences par type, √©tat et ville,\n",
    "- `city_freq_in_state` : fr√©quence conditionnelle ville / √©tat.\n",
    "\n",
    "**Important :** ces fr√©quences (type/state/city) sont calcul√©es de mani√®re coh√©rente\n",
    "par rapport aux **observations pr√©sentes dans `train_fit`** (et non simplement sur `stores.csv`),\n",
    "ce qui refl√®te mieux la distribution r√©elle des donn√©es d‚Äôentra√Ænement.\n",
    "\n",
    "Les variables sont stock√©es sous forme de dictionnaires\n",
    "et inject√©es via `map()` (plus l√©ger qu‚Äôun `merge` complet).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.3 Variables temporelles externes : transactions et p√©trole\n",
    "\n",
    "#### a) Activit√© des magasins : `transactions_roll14` (avec *feature gap*)\n",
    "\n",
    "La variable `transactions_roll14` est calcul√©e √† partir de `transactions.csv`\n",
    "comme une moyenne mobile sur 14 jours.\n",
    "\n",
    "Dans un contexte strict sans retard, on aurait :\n",
    "\n",
    "$\n",
    "\\text{transactions\\_roll14}(t)\n",
    "= \\frac{1}{14} \\sum_{j=1}^{14} \\text{transactions}_{t-j}\n",
    "$\n",
    "\n",
    "Cependant, afin de simuler un contexte r√©aliste de disponibilit√©,\n",
    "nous introduisons un **retard** param√©tr√© par `feature_gap_days = G`.\n",
    "\n",
    "La moyenne mobile devient :\n",
    "\n",
    "$\n",
    "\\text{transactions\\_roll14}(t)\n",
    "= \\frac{1}{14} \\sum_{j=1}^{14} \\text{transactions}_{t-(G+j)}\n",
    "$\n",
    "\n",
    "Autrement dit, si \\(G = 3\\), on utilise les transactions entre \\(t-4\\) et \\(t-17\\),\n",
    "et on exclut les 3 jours les plus r√©cents.\n",
    "\n",
    "Le pipeline conserve :\n",
    "- une s√©rie index√©e `(store_nbr, date) ‚Üí transactions_roll14`,\n",
    "- la **derni√®re valeur connue par magasin**, utilis√©e comme fallback si la date est future.\n",
    "\n",
    "**Remarque pratique :**  \n",
    "`transactions_roll14` est une variable **au niveau magasin**.  \n",
    "Donc pour une m√™me date \\(t\\) et un m√™me magasin, elle est **identique pour tous les produits** du magasin.\n",
    "Cette r√©p√©tition est normale car la feature d√©crit l‚Äôactivit√© globale du store.\n",
    "\n",
    "---\n",
    "\n",
    "#### b) Prix du p√©trole : `oil_roll14` (sans gap additionnel)\n",
    "\n",
    "Pour le p√©trole, nous conservons une logique standard (sans *feature gap* suppl√©mentaire) :\n",
    "\n",
    "$\n",
    "\\text{oil\\_roll14}(t)\n",
    "= \\frac{1}{14} \\sum_{j=1}^{14} \\text{prix}_{t-j}\n",
    "$\n",
    "\n",
    "Les valeurs manquantes sont propag√©es (`ffill`) avant calcul.\n",
    "\n",
    "Pour les dates futures, le pipeline utilise une **valeur de secours** :\n",
    "la moyenne des 30 derniers jours observ√©s.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.4 Jours f√©ri√©s et √©v√©nements\n",
    "\n",
    "Les jours f√©ri√©s sont trait√©s selon une **logique hi√©rarchique** :\n",
    "\n",
    "- niveau **national** (tous les magasins),\n",
    "- niveau **r√©gional** (√©tat),\n",
    "- niveau **local** (ville).\n",
    "\n",
    "Les jours transf√©r√©s (`transferred`) sont repositionn√©s √† leur date effective.\n",
    "Le pipeline construit :\n",
    "\n",
    "- un indicateur national par date,\n",
    "- des overrides par `(store_nbr, date)` pour les niveaux r√©gional et local.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 √âtape TRANSFORM : application aux nouvelles donn√©es\n",
    "\n",
    "L‚Äô√©tape `transform()` est utilis√©e sur :\n",
    "\n",
    "- les donn√©es d‚Äôentra√Ænement enrichies,\n",
    "- les donn√©es de validation,\n",
    "- les donn√©es de test,\n",
    "- les observations unitaires (tests unitaires, production).\n",
    "\n",
    "#### R√®gle essentielle\n",
    "\n",
    "Lors du `transform()` :\n",
    "- **aucune statistique n‚Äôest recalcul√©e √† partir des nouvelles donn√©es**,\n",
    "- seules les informations apprises pendant le `fit()` sont utilis√©es.\n",
    "\n",
    "Cela garantit une parfaite coh√©rence entre entra√Ænement et d√©ploiement.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.1 Gestion des dates futures et valeurs manquantes\n",
    "\n",
    "Pour les dates jamais observ√©es :\n",
    "\n",
    "- `transactions_roll14`  \n",
    "  - lookup si disponible via la s√©rie index√©e,\n",
    "  - sinon **derni√®re valeur connue du magasin**,\n",
    "  - sinon 0.\n",
    "\n",
    "- `oil_roll14`  \n",
    "  - lookup si disponible,\n",
    "  - sinon **moyenne des 30 derniers jours observ√©s**.\n",
    "\n",
    "- jours f√©ri√©s  \n",
    "  - indicateurs fix√©s √† 0 si absents.\n",
    "\n",
    "Ces choix correspondent √† des hypoth√®ses r√©alistes de mise en production :\n",
    "on n‚Äôutilise jamais d‚Äôinformations futures inconnues.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 Variables de s√©ries temporelles : lags et moyennes glissantes (avec *feature gap*)\n",
    "\n",
    "#### 1.4.1 Motivation\n",
    "\n",
    "Les ventes pr√©sentent une forte d√©pendance temporelle :\n",
    "effets hebdomadaires, saisonnalit√©, inertie de la demande.\n",
    "\n",
    "Pour capturer cette dynamique, on cr√©e :\n",
    "\n",
    "- **lags** : ventes observ√©es \\(k\\) jours avant,\n",
    "- **rolling means** : moyenne des ventes sur une fen√™tre pass√©e.\n",
    "\n",
    "Chaque couple `(store_nbr, item_nbr)` est trait√© comme une s√©rie temporelle ind√©pendante.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4.2 Anti-leakage : r√®gle fondamentale + retard d‚Äôacc√®s\n",
    "\n",
    "Pour pr√©dire √† la date \\(t\\), seules les informations **ant√©rieures √† \\(t\\)** sont autoris√©es,\n",
    "et nous imposons en plus un retard de disponibilit√© de \\(G = \\text{feature\\_gap\\_days}\\).\n",
    "\n",
    "La moyenne glissante sur \\(w\\) jours devient :\n",
    "\n",
    "$\n",
    "\\text{rolling}_w(t)\n",
    "= \\frac{1}{w} \\sum_{j=1}^{w} y_{t-(G+j)}\n",
    "$\n",
    "\n",
    "Et un lag \\(k\\) devient :\n",
    "\n",
    "$\n",
    "\\text{lag}_k(t) = y_{t-(G+k)}\n",
    "$\n",
    "\n",
    "Ainsi, si \\(G = 3\\) :\n",
    "- `sales_roll14(t)` utilise les ventes de \\(t-4\\) √† \\(t-17\\),\n",
    "- `sales_lag28(t)` utilise la vente √† \\(t-31\\).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4.3 Gestion des combinaisons absentes et choix des valeurs\n",
    "\n",
    "Les s√©ries `(store, item)` peuvent √™tre tr√®s **spars√©es** :\n",
    "certaines combinaisons n‚Äôont pas de ventes observ√©es dans l‚Äôhistorique r√©cent.\n",
    "\n",
    "Dans ce cas, plut√¥t que de remplacer par 0 (qui pourrait √™tre interpr√©t√© comme une vraie absence de vente),\n",
    "le pipeline conserve :\n",
    "- `sales_lag28` et `sales_roll14` sous forme de **NaN** lorsque la valeur est indisponible,\n",
    "- et fournit des indicateurs :\n",
    "  - `sales_lag28_avail` (disponible ou non),\n",
    "  - `sales_roll14_cnt` (nombre de jours effectivement observ√©s sur 14),\n",
    "  - `sales_roll14_frac` (proportion de jours observ√©s).\n",
    "\n",
    "üëâ Cette strat√©gie est plus fid√®le aux donn√©es : elle distingue ‚Äú0 vente‚Äù de ‚Äúhistorique manquant‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 R√¥le du gap temporel (√©valuation)\n",
    "\n",
    "Un **gap de split** peut √™tre ins√©r√© entre train et test pour renforcer la s√©paration temporelle\n",
    "et √©viter que les mod√®les ne profitent d‚Äôune continuit√© trop directe entre p√©riodes.\n",
    "\n",
    "Il s‚Äôagit d‚Äôune bonne pratique d‚Äô√©valuation (simulation r√©aliste),\n",
    "ind√©pendante du *feature gap* utilis√© dans la construction des variables temporelles.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6 R√©sum√©\n",
    "\n",
    "- Le Feature Engineering est enti√®rement **centralis√©** dans un pipeline unique.\n",
    "- Le `fit()` apprend uniquement √† partir de l‚Äôhistorique `train_fit`.\n",
    "- Le `transform()` applique les m√™mes r√®gles aux nouvelles donn√©es sans recalculer de statistiques.\n",
    "- Les variables temporelles de ventes (`lag`, `rolling`) et `transactions_roll14` int√®grent un **feature gap** param√©trable.\n",
    "- `oil_roll14` reste calcul√© de mani√®re standard (shift + rolling), avec fallback r√©aliste.\n",
    "- Les valeurs manquantes sur les s√©ries de ventes sont conserv√©es en **NaN**, accompagn√©es d‚Äôindicateurs de disponibilit√©.\n",
    "\n",
    "üëâ Cette architecture garantit une m√©thodologie solide,\n",
    "reproductible et compatible avec un d√©ploiement r√©el.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "9321075e-3f79-433b-a923-04ccea52f3d5",
    "_uuid": "5be462a3-f710-44a7-97a2-0e80e0c47d7f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-13T14:24:34.687857Z",
     "iopub.status.busy": "2026-01-13T14:24:34.687490Z",
     "iopub.status.idle": "2026-01-13T14:24:34.744050Z",
     "shell.execute_reply": "2026-01-13T14:24:34.743061Z",
     "shell.execute_reply.started": "2026-01-13T14:24:34.687826Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "class FavoritaFeaturePipeline:\n",
    "    \"\"\"\n",
    "    Pipeline Fit/Transform pour cr√©er les features Favorita sans fuite d'information.\n",
    "\n",
    "    - feature_gap_days = retard d'acc√®s aux donn√©es de ventes (et transactions si on le souhaite)\n",
    "      Exemple feature_gap_days=3 :\n",
    "        sales_roll14(t) utilise t-4..t-17\n",
    "        sales_lag28(t)  utilise t-31\n",
    "        transactions_roll14(t) utilise t-4..t-17 (dans ce pipeline)\n",
    "    - oil : on laisse le calcul standard (shift(1) rolling 14), sans gap additionnel.\n",
    "    - IMPORTANT : on garde NaN pour sales_lag28 et sales_roll14 quand l'historique n'existe pas.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, sales_history_days=120, feature_gap_days=0, verbose=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.sales_history_days = int(sales_history_days)\n",
    "        self.feature_gap_days = int(feature_gap_days)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # learned objects\n",
    "        self.store_freq = None\n",
    "        self.item_freq  = None\n",
    "\n",
    "        self.items_fe = None\n",
    "        self.store_maps = {}\n",
    "\n",
    "        self.trans_s = None\n",
    "        self.last_roll_by_store = None\n",
    "\n",
    "        self.oil_s = None\n",
    "\n",
    "        self.holiday_nat_maps = {}\n",
    "        self.holiday_override_maps = {}\n",
    "\n",
    "        # sales lookup history\n",
    "        self.sales_s = None\n",
    "        self.sales_target_col = None\n",
    "        self.sales_history_max_date = None\n",
    "\n",
    "    # ---------------------------\n",
    "    # Utils\n",
    "    # ---------------------------\n",
    "    def _log(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_datetime(df, col=\"date\"):\n",
    "        df[col] = pd.to_datetime(df[col]).dt.normalize()\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_key_store_date(store_series, date_series):\n",
    "        \"\"\"cl√© num√©rique compacte store-date\"\"\"\n",
    "        ymd = date_series.dt.year * 10000 + date_series.dt.month * 100 + date_series.dt.day\n",
    "        return (store_series.astype(\"int32\") * 1_000_000 + ymd.astype(\"int32\")).astype(\"int64\")\n",
    "\n",
    "    @staticmethod\n",
    "    def add_target(df, y_col=\"unit_sales\"):\n",
    "        \"\"\"\n",
    "        Optionnel : pr√©pare la cible si elle n'existe pas.\n",
    "        - unit_sales_clean = clip des n√©gatives √† 0\n",
    "        - unit_sales_log   = log1p(unit_sales_clean)\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        if \"unit_sales_clean\" not in df.columns:\n",
    "            df[\"unit_sales_clean\"] = pd.to_numeric(df[y_col], errors=\"coerce\").fillna(0).clip(lower=0).astype(\"float32\")\n",
    "        if \"unit_sales_log\" not in df.columns:\n",
    "            df[\"unit_sales_log\"] = np.log1p(df[\"unit_sales_clean\"]).astype(\"float32\")\n",
    "        return df\n",
    "\n",
    "    # ---------------------------\n",
    "    # FIT\n",
    "    # ---------------------------\n",
    "    def fit(self, history_df):\n",
    "        \"\"\"\n",
    "        history_df : dataframe historique (train_fit) contenant\n",
    "        date/store/item + cible (unit_sales_clean ou unit_sales_log)\n",
    "        \"\"\"\n",
    "        history_df = history_df.copy()\n",
    "        history_df = self._ensure_datetime(history_df, \"date\")\n",
    "\n",
    "        self._log(\"üîß FIT...\")\n",
    "\n",
    "        # ---- d√©tecter cible\n",
    "        if \"unit_sales_log\" in history_df.columns:\n",
    "            target_col = \"unit_sales_log\"\n",
    "        elif \"unit_sales_clean\" in history_df.columns:\n",
    "            target_col = \"unit_sales_clean\"\n",
    "        else:\n",
    "            raise ValueError(\"Il faut une colonne cible : unit_sales_clean ou unit_sales_log (tu peux appeler add_target avant).\")\n",
    "\n",
    "        self.sales_target_col = target_col\n",
    "\n",
    "        # forcer types\n",
    "        history_df[\"store_nbr\"] = history_df[\"store_nbr\"].astype(\"int16\")\n",
    "        history_df[\"item_nbr\"]  = history_df[\"item_nbr\"].astype(\"int32\")\n",
    "\n",
    "        # ---- freq enc (appris sur history_df)\n",
    "        self.store_freq = history_df[\"store_nbr\"].value_counts(normalize=True)\n",
    "        self.item_freq  = history_df[\"item_nbr\"].value_counts(normalize=True)\n",
    "\n",
    "        # ---- Sales lookup (limit√© sur sales_history_days)\n",
    "        max_d = history_df[\"date\"].max()\n",
    "        min_keep = max_d - pd.Timedelta(days=self.sales_history_days)\n",
    "\n",
    "        hs = history_df.loc[history_df[\"date\"] >= min_keep, [\"store_nbr\", \"item_nbr\", \"date\", target_col]].copy()\n",
    "        hs[\"store_nbr\"] = hs[\"store_nbr\"].astype(\"int16\")\n",
    "        hs[\"item_nbr\"]  = hs[\"item_nbr\"].astype(\"int32\")\n",
    "        hs[target_col]  = hs[target_col].astype(\"float32\")\n",
    "\n",
    "        hs = hs.drop_duplicates([\"store_nbr\", \"item_nbr\", \"date\"], keep=\"last\")\n",
    "        self.sales_history_max_date = hs[\"date\"].max()\n",
    "        self.sales_s = hs.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[target_col]\n",
    "\n",
    "        del hs\n",
    "        gc.collect()\n",
    "\n",
    "        # ---- ITEMS\n",
    "        items = pd.read_csv(self.data_dir + \"/items.csv\")\n",
    "        items[\"item_nbr\"] = items[\"item_nbr\"].astype(\"int32\")\n",
    "        items[\"family\"] = items[\"family\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
    "        items[\"class\"]  = items[\"class\"].fillna(-1).astype(\"int16\").astype(str)\n",
    "        items[\"perishable\"] = items[\"perishable\"].fillna(0).astype(\"int8\")\n",
    "\n",
    "        fam_freq = items[\"family\"].value_counts(normalize=True)\n",
    "        cls_freq = items[\"class\"].value_counts(normalize=True)\n",
    "\n",
    "        items[\"family_freq_items\"] = items[\"family\"].map(fam_freq).astype(\"float32\")\n",
    "        items[\"class_freq_items\"]  = items[\"class\"].map(cls_freq).astype(\"float32\")\n",
    "\n",
    "        pair_counts   = items.groupby([\"family\", \"class\"]).size()\n",
    "        family_counts = items.groupby(\"family\").size()\n",
    "\n",
    "        items[\"class_freq_in_family\"] = (\n",
    "            items.set_index([\"family\", \"class\"]).index.map(pair_counts) /\n",
    "            items[\"family\"].map(family_counts).to_numpy()\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        self.items_fe = items[[\n",
    "            \"item_nbr\", \"perishable\", \"family_freq_items\", \"class_freq_items\", \"class_freq_in_family\"\n",
    "        ]].drop_duplicates(\"item_nbr\")\n",
    "\n",
    "        del items\n",
    "        gc.collect()\n",
    "\n",
    "        # ---- STORES (freq apprises sur les OBS history_df)\n",
    "        stores = pd.read_csv(self.data_dir + \"/stores.csv\")\n",
    "        stores[\"store_nbr\"] = stores[\"store_nbr\"].astype(\"int16\")\n",
    "        stores[\"type\"]  = stores[\"type\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
    "        stores[\"state\"] = stores[\"state\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
    "        stores[\"city\"]  = stores[\"city\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
    "        stores[\"cluster\"] = stores[\"cluster\"].fillna(-1).astype(\"int16\")\n",
    "\n",
    "        hist_loc = history_df[[\"store_nbr\"]].merge(\n",
    "            stores[[\"store_nbr\", \"type\", \"state\", \"city\", \"cluster\"]],\n",
    "            on=\"store_nbr\", how=\"left\"\n",
    "        )\n",
    "\n",
    "        type_freq  = hist_loc[\"type\"].value_counts(normalize=True)\n",
    "        state_freq = hist_loc[\"state\"].value_counts(normalize=True)\n",
    "        city_freq  = hist_loc[\"city\"].value_counts(normalize=True)\n",
    "\n",
    "        stores[\"type_freq\"]  = stores[\"type\"].map(type_freq).fillna(0).astype(\"float32\")\n",
    "        stores[\"state_freq\"] = stores[\"state\"].map(state_freq).fillna(0).astype(\"float32\")\n",
    "        stores[\"city_freq\"]  = stores[\"city\"].map(city_freq).fillna(0).astype(\"float32\")\n",
    "\n",
    "        pair_counts  = hist_loc.groupby([\"state\", \"city\"]).size()\n",
    "        state_counts = hist_loc.groupby(\"state\").size()\n",
    "\n",
    "        st = stores.set_index(\"store_nbr\")[[\"state\", \"city\"]]\n",
    "        num = st.index.map(lambda sn: pair_counts.get((st.loc[sn, \"state\"], st.loc[sn, \"city\"]), 0))\n",
    "        den = st[\"state\"].map(state_counts).fillna(0).to_numpy()\n",
    "\n",
    "        stores[\"city_freq_in_state\"] = np.where(den > 0, np.array(num) / den, 0.0).astype(\"float32\")\n",
    "\n",
    "        self.store_maps = {\n",
    "            \"cluster\": stores.set_index(\"store_nbr\")[\"cluster\"].to_dict(),\n",
    "            \"type_freq\": stores.set_index(\"store_nbr\")[\"type_freq\"].to_dict(),\n",
    "            \"state_freq\": stores.set_index(\"store_nbr\")[\"state_freq\"].to_dict(),\n",
    "            \"city_freq\": stores.set_index(\"store_nbr\")[\"city_freq\"].to_dict(),\n",
    "            \"city_freq_in_state\": stores.set_index(\"store_nbr\")[\"city_freq_in_state\"].to_dict(),\n",
    "        }\n",
    "\n",
    "        del stores, hist_loc\n",
    "        gc.collect()\n",
    "\n",
    "        # ---- TRANSACTIONS roll14 (avec le m√™me gap que ventes)\n",
    "        tr = pd.read_csv(self.data_dir + \"/transactions.csv\", parse_dates=[\"date\"])\n",
    "        tr[\"date\"] = pd.to_datetime(tr[\"date\"]).dt.normalize()\n",
    "        tr[\"store_nbr\"] = tr[\"store_nbr\"].astype(\"int16\")\n",
    "        tr[\"transactions\"] = pd.to_numeric(tr[\"transactions\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "        tr = (tr.groupby([\"store_nbr\", \"date\"], as_index=False)\n",
    "                .agg(transactions=(\"transactions\", \"sum\"))\n",
    "                .sort_values([\"store_nbr\", \"date\"]))\n",
    "\n",
    "        G = self.feature_gap_days\n",
    "        tr[\"transactions_roll14\"] = (\n",
    "            tr.groupby(\"store_nbr\")[\"transactions\"]\n",
    "              .transform(lambda x: x.shift(1 + G).rolling(14, min_periods=1).mean())\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        self.trans_s = tr.set_index([\"store_nbr\", \"date\"])[\"transactions_roll14\"]\n",
    "        self.last_roll_by_store = tr.groupby(\"store_nbr\")[\"transactions_roll14\"].last()\n",
    "\n",
    "        del tr\n",
    "        gc.collect()\n",
    "\n",
    "        # ---- OIL roll14 (standard, sans gap additionnel)\n",
    "        oil = pd.read_csv(self.data_dir + \"/oil.csv\", parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "        oil[\"date\"] = pd.to_datetime(oil[\"date\"]).dt.normalize()\n",
    "        oil[\"dcoilwtico\"] = pd.to_numeric(oil[\"dcoilwtico\"], errors=\"coerce\").ffill()\n",
    "        oil[\"oil_roll14\"] = oil[\"dcoilwtico\"].shift(1).rolling(14, min_periods=1).mean().astype(\"float32\")\n",
    "        self.oil_s = oil.set_index(\"date\")[\"oil_roll14\"]\n",
    "\n",
    "        del oil\n",
    "        gc.collect()\n",
    "\n",
    "        # ---- HOLIDAYS (national + override store)\n",
    "        hol = pd.read_csv(self.data_dir + \"/holidays_events.csv\")\n",
    "        hol[\"date\"] = pd.to_datetime(hol[\"date\"]).dt.normalize()\n",
    "        for col in [\"type\", \"locale\", \"locale_name\", \"description\"]:\n",
    "            hol[col] = hol[col].fillna(\"\").astype(str).str.strip()\n",
    "        hol[\"transferred\"] = hol[\"transferred\"].fillna(False).astype(int)\n",
    "\n",
    "        hol[\"event_key\"] = hol[\"description\"] + \" | \" + hol[\"locale\"] + \" | \" + hol[\"locale_name\"]\n",
    "\n",
    "        transfer_map = (\n",
    "            hol.loc[hol[\"type\"] == \"Transfer\", [\"event_key\", \"date\"]]\n",
    "               .drop_duplicates(\"event_key\")\n",
    "               .set_index(\"event_key\")[\"date\"]\n",
    "               .to_dict()\n",
    "        )\n",
    "\n",
    "        hol[\"observed_date\"] = hol[\"date\"]\n",
    "        mask_moved = (hol[\"type\"] == \"Holiday\") & (hol[\"transferred\"] == 1)\n",
    "        hol.loc[mask_moved, \"observed_date\"] = hol.loc[mask_moved, \"event_key\"].map(transfer_map)\n",
    "        hol[\"observed_date\"] = hol[\"observed_date\"].fillna(hol[\"date\"])\n",
    "\n",
    "        hol[\"f_holiday\"] = (hol[\"type\"] == \"Holiday\").astype(\"int8\")\n",
    "\n",
    "        stores_loc = pd.read_csv(self.data_dir + \"/stores.csv\")\n",
    "        stores_loc[\"store_nbr\"] = stores_loc[\"store_nbr\"].astype(\"int16\")\n",
    "        stores_loc[\"city\"]  = stores_loc[\"city\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
    "        stores_loc[\"state\"] = stores_loc[\"state\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
    "        store_loc = stores_loc[[\"store_nbr\", \"city\", \"state\"]].drop_duplicates(\"store_nbr\")\n",
    "\n",
    "        national = (hol[hol[\"locale\"] == \"National\"]\n",
    "                    .groupby(\"observed_date\", as_index=False)\n",
    "                    .agg(nat_h=(\"f_holiday\", lambda s: int(s.sum() > 0)))\n",
    "                    .rename(columns={\"observed_date\": \"date\"}))\n",
    "\n",
    "        self.holiday_nat_maps = {\"nat_h\": dict(zip(national[\"date\"], national[\"nat_h\"]))}\n",
    "\n",
    "        regional = hol[hol[\"locale\"] == \"Regional\"].copy()\n",
    "        regional[\"date\"] = regional[\"observed_date\"]\n",
    "        regional = regional.drop(columns=[\"observed_date\"]).merge(\n",
    "            store_loc, left_on=\"locale_name\", right_on=\"state\", how=\"inner\"\n",
    "        )\n",
    "        regional = (regional.groupby([\"date\", \"store_nbr\"], as_index=False)\n",
    "                    .agg(h=(\"f_holiday\", lambda s: int(s.sum() > 0))))\n",
    "\n",
    "        local = hol[hol[\"locale\"] == \"Local\"].copy()\n",
    "        local[\"date\"] = local[\"observed_date\"]\n",
    "        local = local.drop(columns=[\"observed_date\"]).merge(\n",
    "            store_loc, left_on=\"locale_name\", right_on=\"city\", how=\"inner\"\n",
    "        )\n",
    "        local = (local.groupby([\"date\", \"store_nbr\"], as_index=False)\n",
    "                 .agg(h=(\"f_holiday\", lambda s: int(s.sum() > 0))))\n",
    "\n",
    "        over = pd.concat([regional, local], ignore_index=True).groupby([\"date\", \"store_nbr\"], as_index=False).max()\n",
    "        over[\"key\"] = self._make_key_store_date(over[\"store_nbr\"], over[\"date\"])\n",
    "        self.holiday_override_maps = {\"h\": dict(zip(over[\"key\"], over[\"h\"]))}\n",
    "\n",
    "        del hol, stores_loc, store_loc, national, regional, local, over\n",
    "        gc.collect()\n",
    "\n",
    "        self._log(\"‚úÖ FIT termin√©.\")\n",
    "        return self\n",
    "\n",
    "    # ---------------------------\n",
    "    # TRANSFORM\n",
    "    # ---------------------------\n",
    "    def transform(self, df_new):\n",
    "        df = df_new.copy()\n",
    "        df = self._ensure_datetime(df, \"date\")\n",
    "\n",
    "        # types\n",
    "        df[\"store_nbr\"] = df[\"store_nbr\"].astype(\"int16\")\n",
    "        df[\"item_nbr\"]  = df[\"item_nbr\"].astype(\"int32\")\n",
    "\n",
    "        # ---- date feats\n",
    "        d = df[\"date\"]\n",
    "        df[\"year\"]  = d.dt.year.astype(\"int16\")\n",
    "        df[\"month\"] = d.dt.month.astype(\"int8\")\n",
    "        df[\"day\"]   = d.dt.day.astype(\"int8\")\n",
    "        df[\"dow\"]   = d.dt.dayofweek.astype(\"int8\")\n",
    "        df[\"is_weekend\"] = (df[\"dow\"] >= 5).astype(\"int8\")\n",
    "\n",
    "        # ---- promo (robuste)\n",
    "        if \"onpromotion\" in df.columns:\n",
    "            promo = df[\"onpromotion\"]\n",
    "            if promo.dtype == \"O\":\n",
    "                promo_bool = promo.astype(str).str.lower().isin([\"true\", \"1\", \"t\", \"yes\"])\n",
    "                promo_bool = promo_bool.where(promo.notna(), False)\n",
    "                promo = promo_bool\n",
    "            df[\"onpromo\"] = promo.fillna(False).astype(\"int8\")\n",
    "        else:\n",
    "            df[\"onpromo\"] = np.int8(0)\n",
    "\n",
    "        # ---- freq enc\n",
    "        df[\"store_freq\"] = pd.to_numeric(df[\"store_nbr\"].map(self.store_freq), errors=\"coerce\").fillna(0).astype(\"float32\")\n",
    "        df[\"item_freq\"]  = pd.to_numeric(df[\"item_nbr\"].map(self.item_freq),  errors=\"coerce\").fillna(0).astype(\"float32\")\n",
    "\n",
    "        # ---- items merge\n",
    "        df = df.merge(self.items_fe, on=\"item_nbr\", how=\"left\")\n",
    "        df[\"perishable\"] = pd.to_numeric(df[\"perishable\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "        for c in [\"family_freq_items\", \"class_freq_items\", \"class_freq_in_family\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(\"float32\")\n",
    "\n",
    "        # ---- stores maps\n",
    "        df[\"cluster\"] = pd.to_numeric(df[\"store_nbr\"].map(self.store_maps[\"cluster\"]), errors=\"coerce\").fillna(-1).astype(\"int16\")\n",
    "        for c in [\"type_freq\", \"state_freq\", \"city_freq\", \"city_freq_in_state\"]:\n",
    "            df[c] = pd.to_numeric(df[\"store_nbr\"].map(self.store_maps[c]), errors=\"coerce\").fillna(0).astype(\"float32\")\n",
    "\n",
    "        # ---- transactions_roll14\n",
    "        keys = pd.MultiIndex.from_frame(df[[\"store_nbr\", \"date\"]])\n",
    "        vals = self.trans_s.reindex(keys).to_numpy(dtype=\"float32\")\n",
    "        df[\"transactions_roll14\"] = pd.Series(vals, index=df.index)\n",
    "\n",
    "        last_vals = pd.to_numeric(df[\"store_nbr\"].map(self.last_roll_by_store), errors=\"coerce\")\n",
    "        df[\"transactions_roll14\"] = df[\"transactions_roll14\"].fillna(last_vals).fillna(0).astype(\"float32\")\n",
    "\n",
    "        # ---- oil_roll14\n",
    "        oil_vals = self.oil_s.reindex(df[\"date\"]).to_numpy(dtype=\"float32\")\n",
    "        df[\"oil_roll14\"] = pd.Series(oil_vals, index=df.index)\n",
    "        fallback_oil = float(self.oil_s.tail(30).mean()) if self.oil_s is not None and len(self.oil_s) else 0.0\n",
    "        df[\"oil_roll14\"] = df[\"oil_roll14\"].fillna(fallback_oil).astype(\"float32\")\n",
    "\n",
    "        # ---- holidays\n",
    "        df[\"is_holiday_effective\"] = df[\"date\"].map(self.holiday_nat_maps[\"nat_h\"]).fillna(0).astype(\"int8\")\n",
    "        k = self._make_key_store_date(df[\"store_nbr\"], df[\"date\"])\n",
    "        h_over = pd.Series(k).map(self.holiday_override_maps[\"h\"]).fillna(0).to_numpy()\n",
    "        df[\"is_holiday_effective\"] = np.maximum(df[\"is_holiday_effective\"].to_numpy(), h_over).astype(\"int8\")\n",
    "\n",
    "        # ============================\n",
    "        # SALES LAG28 + ROLL14 (NaN + gap)\n",
    "        # ============================\n",
    "        G = self.feature_gap_days\n",
    "\n",
    "        # --- LAG 28 (d√©cal√©)\n",
    "        L = 28\n",
    "        lag_idx = pd.MultiIndex.from_frame(pd.DataFrame({\n",
    "            \"store_nbr\": df[\"store_nbr\"].astype(\"int16\"),\n",
    "            \"item_nbr\":  df[\"item_nbr\"].astype(\"int32\"),\n",
    "            \"date\":      df[\"date\"] - pd.Timedelta(days=L + G)\n",
    "        }))\n",
    "        lag_vals = self.sales_s.reindex(lag_idx).to_numpy(dtype=\"float32\")\n",
    "\n",
    "        df[f\"sales_lag{L}\"] = pd.Series(lag_vals, index=df.index).astype(\"float32\")  # ‚úÖ garde NaN\n",
    "        df[f\"sales_lag{L}_avail\"] = (~pd.isna(lag_vals)).astype(\"int8\")\n",
    "\n",
    "        # --- ROLL 14 (t-(G+1) .. t-(G+14))\n",
    "        W = 14\n",
    "        acc = np.zeros(len(df), dtype=\"float32\")\n",
    "        cnt = np.zeros(len(df), dtype=\"int16\")\n",
    "\n",
    "        for j in range(1, W + 1):\n",
    "            r_idx = pd.MultiIndex.from_frame(pd.DataFrame({\n",
    "                \"store_nbr\": df[\"store_nbr\"].astype(\"int16\"),\n",
    "                \"item_nbr\":  df[\"item_nbr\"].astype(\"int32\"),\n",
    "                \"date\":      df[\"date\"] - pd.Timedelta(days=G + j)\n",
    "            }))\n",
    "            vals = self.sales_s.reindex(r_idx).to_numpy(dtype=\"float32\")\n",
    "            mask = ~pd.isna(vals)\n",
    "            acc[mask] += vals[mask]\n",
    "            cnt[mask] += 1\n",
    "\n",
    "        roll = np.full(len(df), np.nan, dtype=\"float32\")\n",
    "        np.divide(acc, cnt, out=roll, where=cnt > 0)  # ‚úÖ pas de warning\n",
    "\n",
    "        df[f\"sales_roll{W}\"] = pd.Series(roll, index=df.index).astype(\"float32\")\n",
    "        df[f\"sales_roll{W}_cnt\"] = cnt.astype(\"int16\")\n",
    "        df[f\"sales_roll{W}_frac\"] = (cnt / float(W)).astype(\"float32\")\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f1aa3ae-b696-451d-aa70-5f6eea540b56",
    "_uuid": "d353c916-dd1f-46cb-90f1-be258a52b9ca",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 2.1. Application du pipeline sur les donn√©es d'entrainement (d√©j√† split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "57e9c919-288a-4a79-8040-eee47e36bdac",
    "_uuid": "faea66d9-1a61-4575-af8d-56c7a4505852",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-13T14:25:09.085037Z",
     "iopub.status.busy": "2026-01-13T14:25:09.084649Z",
     "iopub.status.idle": "2026-01-13T14:26:22.462029Z",
     "shell.execute_reply": "2026-01-13T14:26:22.460727Z",
     "shell.execute_reply.started": "2026-01-13T14:25:09.085004Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7078551, 5) 2017-05-24 00:00:00 2017-07-29 00:00:00\n",
      "Gap  : (327256, 5) 2017-07-30 00:00:00 2017-08-01 00:00:00\n",
      "Test : (1461581, 5) 2017-08-02 00:00:00 2017-08-15 00:00:00\n",
      "üîß FIT...\n",
      "‚úÖ FIT termin√©.\n",
      "Transform termin√©\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train[\"date\"] = pd.to_datetime(train[\"date\"]).dt.normalize()\n",
    "\n",
    "TOTAL_DAYS = 84\n",
    "TEST_DAYS  = 14\n",
    "GAP_DAYS   = 3\n",
    "\n",
    "max_date = train[\"date\"].max()\n",
    "start_84 = max_date - pd.Timedelta(days=TOTAL_DAYS - 1)\n",
    "\n",
    "train_84 = train.loc[train[\"date\"] >= start_84].copy()\n",
    "\n",
    "# --- TEST ---\n",
    "end_test = max_date\n",
    "start_test = end_test - pd.Timedelta(days=TEST_DAYS - 1)\n",
    "\n",
    "# --- GAP (juste avant test) ---\n",
    "end_gap = start_test - pd.Timedelta(days=1)\n",
    "start_gap = end_gap - pd.Timedelta(days=GAP_DAYS - 1)\n",
    "\n",
    "# --- TRAIN = le reste ---\n",
    "train_fit = train_84.loc[train_84[\"date\"] < start_gap].copy()\n",
    "gap_df    = train_84.loc[(train_84[\"date\"] >= start_gap) & (train_84[\"date\"] <= end_gap)].copy()\n",
    "test_df   = train_84.loc[(train_84[\"date\"] >= start_test) & (train_84[\"date\"] <= end_test)].copy()\n",
    "\n",
    "print(\"Train:\", train_fit.shape, train_fit[\"date\"].min(), train_fit[\"date\"].max())\n",
    "print(\"Gap  :\", gap_df.shape, gap_df[\"date\"].min(), gap_df[\"date\"].max())\n",
    "print(\"Test :\", test_df.shape, test_df[\"date\"].min(), test_df[\"date\"].max())\n",
    "\n",
    "# FIT sur train uniquement\n",
    "pipe = FavoritaFeaturePipeline(\n",
    "    \"/kaggle/working/favorita_data\",\n",
    "    sales_history_days=120,\n",
    "    feature_gap_days=3,\n",
    "    verbose=True\n",
    ")\n",
    "# 1) cr√©er la cible sur train_fit et test_df (et gap_df aussi si on veut)\n",
    "train_fit = FavoritaFeaturePipeline.add_target(train_fit, y_col=\"unit_sales\")\n",
    "test_df   = FavoritaFeaturePipeline.add_target(test_df,   y_col=\"unit_sales\")\n",
    "gap_df    = FavoritaFeaturePipeline.add_target(gap_df,    y_col=\"unit_sales\")\n",
    "\n",
    " \n",
    "# 2) fit\n",
    "pipe.fit(train_fit)\n",
    "\n",
    "\n",
    "\n",
    "# TRANSFORM\n",
    "X_train = pipe.transform(train_fit)\n",
    "X_gap   = pipe.transform(gap_df)\n",
    "X_test  = pipe.transform(test_df)\n",
    "print(\"Transform termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "60114afe-b63d-4813-96be-15d78341c55c",
    "_uuid": "a8d6c9ed-38b1-4ba3-8ef7-c56133e63214",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-13T14:26:38.658851Z",
     "iopub.status.busy": "2026-01-13T14:26:38.658474Z",
     "iopub.status.idle": "2026-01-13T14:26:38.686847Z",
     "shell.execute_reply": "2026-01-13T14:26:38.685612Z",
     "shell.execute_reply.started": "2026-01-13T14:26:38.658820Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>unit_sales_clean</th>\n",
       "      <th>unit_sales_log</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dow</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>onpromo</th>\n",
       "      <th>store_freq</th>\n",
       "      <th>item_freq</th>\n",
       "      <th>perishable</th>\n",
       "      <th>family_freq_items</th>\n",
       "      <th>class_freq_items</th>\n",
       "      <th>class_freq_in_family</th>\n",
       "      <th>cluster</th>\n",
       "      <th>type_freq</th>\n",
       "      <th>state_freq</th>\n",
       "      <th>city_freq</th>\n",
       "      <th>city_freq_in_state</th>\n",
       "      <th>transactions_roll14</th>\n",
       "      <th>oil_roll14</th>\n",
       "      <th>is_holiday_effective</th>\n",
       "      <th>sales_lag28</th>\n",
       "      <th>sales_lag28_avail</th>\n",
       "      <th>sales_roll14</th>\n",
       "      <th>sales_roll14_cnt</th>\n",
       "      <th>sales_roll14_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>1</td>\n",
       "      <td>99197</td>\n",
       "      <td>2.00</td>\n",
       "      <td>False</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1534.71</td>\n",
       "      <td>48.22</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>1</td>\n",
       "      <td>103520</td>\n",
       "      <td>4.00</td>\n",
       "      <td>False</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>13</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1534.71</td>\n",
       "      <td>48.22</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1534.71</td>\n",
       "      <td>48.22</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>5.00</td>\n",
       "      <td>False</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>13</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1534.71</td>\n",
       "      <td>48.22</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>1</td>\n",
       "      <td>105575</td>\n",
       "      <td>12.00</td>\n",
       "      <td>False</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>13</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1534.71</td>\n",
       "      <td>48.22</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_nbr  item_nbr  unit_sales onpromotion  unit_sales_clean  \\\n",
       "0 2017-05-24          1     99197        2.00       False              2.00   \n",
       "1 2017-05-24          1    103520        4.00       False              4.00   \n",
       "2 2017-05-24          1    103665       -1.00       False              0.00   \n",
       "3 2017-05-24          1    105574        5.00       False              5.00   \n",
       "4 2017-05-24          1    105575       12.00       False             12.00   \n",
       "\n",
       "   unit_sales_log  year  month  day  dow  is_weekend  onpromo  store_freq  \\\n",
       "0            1.10  2017      5   24    2           0        0        0.02   \n",
       "1            1.61  2017      5   24    2           0        0        0.02   \n",
       "2            0.00  2017      5   24    2           0        0        0.02   \n",
       "3            1.79  2017      5   24    2           0        0        0.02   \n",
       "4            2.56  2017      5   24    2           0        0        0.02   \n",
       "\n",
       "   item_freq  perishable  family_freq_items  class_freq_items  \\\n",
       "0       0.00           0               0.33              0.00   \n",
       "1       0.00           0               0.33              0.01   \n",
       "2       0.00           1               0.03              0.00   \n",
       "3       0.00           0               0.33              0.01   \n",
       "4       0.00           0               0.33              0.01   \n",
       "\n",
       "   class_freq_in_family  cluster  type_freq  state_freq  city_freq  \\\n",
       "0                  0.00       13       0.36        0.41       0.40   \n",
       "1                  0.04       13       0.36        0.41       0.40   \n",
       "2                  0.13       13       0.36        0.41       0.40   \n",
       "3                  0.02       13       0.36        0.41       0.40   \n",
       "4                  0.02       13       0.36        0.41       0.40   \n",
       "\n",
       "   city_freq_in_state  transactions_roll14  oil_roll14  is_holiday_effective  \\\n",
       "0                0.95              1534.71       48.22                     1   \n",
       "1                0.95              1534.71       48.22                     1   \n",
       "2                0.95              1534.71       48.22                     1   \n",
       "3                0.95              1534.71       48.22                     1   \n",
       "4                0.95              1534.71       48.22                     1   \n",
       "\n",
       "   sales_lag28  sales_lag28_avail  sales_roll14  sales_roll14_cnt  \\\n",
       "0          NaN                  0           NaN                 0   \n",
       "1          NaN                  0           NaN                 0   \n",
       "2          NaN                  0           NaN                 0   \n",
       "3          NaN                  0           NaN                 0   \n",
       "4          NaN                  0           NaN                 0   \n",
       "\n",
       "   sales_roll14_frac  \n",
       "0               0.00  \n",
       "1               0.00  \n",
       "2               0.00  \n",
       "3               0.00  \n",
       "4               0.00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cecf5d78-e685-46e5-9663-1714b6a9e87d",
    "_uuid": "87fd7e9b-8206-4467-9b9d-cbd1c67aaa91",
    "collapsed": false,
    "execution": {
     "execution_failed": "2026-01-13T05:43:27.620Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5e07731-1611-4540-9702-348f63ef26f0",
    "_uuid": "47bec59b-59ed-442f-937a-aed0a3407782",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 2.1. Tests unitaires du pipeline\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1.1 Pourquoi faire des tests unitaires dans un projet de pr√©vision des ventes ?\n",
    "\n",
    "Dans un projet de pr√©vision **temporelle** (comme Favorita), le risque principal n‚Äôest pas seulement d‚Äôavoir des bugs,\n",
    "mais surtout d‚Äôavoir un pipeline qui :\n",
    "\n",
    "- produit des features incoh√©rentes entre train et test,\n",
    "- introduit une fuite d‚Äôinformation (*data leakage*) sans qu‚Äôon s‚Äôen rende compte,\n",
    "- casse en ‚Äúproduction‚Äù (ex. date future, item rare, store absent, promo manquante),\n",
    "- renvoie des valeurs `NaN`, `inf`, ou des features mal typ√©es.\n",
    "\n",
    "Les **tests unitaires** permettent donc de v√©rifier que le pipeline fonctionne comme un **module fiable** :  \n",
    "> on donne une entr√©e (m√™me tr√®s petite) ‚Üí on obtient une sortie stable, coh√©rente et exploitable par le mod√®le.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1.2 Qu‚Äôappelle-t-on ‚Äútest unitaire‚Äù ici ?\n",
    "\n",
    "Dans ce projet, un test unitaire correspond √† une v√©rification automatique sur une ou plusieurs propri√©t√©s du pipeline, par exemple :\n",
    "\n",
    "- le pipeline cr√©e bien toutes les colonnes attendues,\n",
    "- aucune feature n‚Äôest recalcul√©e √† partir des nouvelles donn√©es (validation/test/production),\n",
    "- les lags et rollings proviennent uniquement de l‚Äôhistorique appris au `fit()`,\n",
    "- les valeurs de secours (*fallbacks*) sont correctes (dates futures),\n",
    "- les features restent num√©riques (et donc compatibles avec le mod√®le).\n",
    "\n",
    "Dans le contexte Favorita, on distingue deux grandes familles de tests unitaires :\n",
    "\n",
    "1) **Tests sur des datasets (train/test)**\n",
    "   - v√©rifier dimensions, colonnes, absence de valeurs impossibles,\n",
    "   - v√©rifier coh√©rence temporelle (split + gap).\n",
    "\n",
    "2) **Tests ‚Äúproduction‚Äù sur une observation unique**\n",
    "   - v√©rifier que le pipeline marche quand on lui donne une seule ligne :  \n",
    "     `(date, store_nbr, item_nbr, onpromotion)`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1.3 Unit test ‚Äúproduction‚Äù : une seule ligne en entr√©e\n",
    "\n",
    "##### Objectif du test\n",
    "\n",
    "En situation r√©elle, le mod√®le re√ßoit souvent une ligne du type :\n",
    "\n",
    "- date = `2017-08-16`\n",
    "- store_nbr = `1`\n",
    "- item_nbr = `96995`\n",
    "- onpromotion = `False`\n",
    "\n",
    "L‚Äôutilisateur ne fournit **jamais** les ventes du jour (la cible) : c‚Äôest justement ce qu‚Äôon veut pr√©dire.\n",
    "\n",
    "Le test unitaire v√©rifie donc que :\n",
    "\n",
    "- le pipeline ne demande pas `unit_sales`,\n",
    "- il enrichit correctement la ligne avec toutes les features,\n",
    "- il g√®re les dates futures sans planter,\n",
    "- il respecte la logique anti-leakage et le **feature gap**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1.4 Comment les features sont-elles calcul√©es dans ce cas ?\n",
    "\n",
    "Le pipeline applique exactement la m√™me logique que pour le test ou la validation.\n",
    "\n",
    "##### A) Features calendrier (toujours disponibles)\n",
    "Calcul√©es directement √† partir de la date :\n",
    "\n",
    "- `year`, `month`, `day`, `dow`, `is_weekend`, etc.\n",
    "\n",
    "> Aucun besoin d‚Äôhistorique.\n",
    "\n",
    "##### B) Promotion\n",
    "Si `onpromotion` est fourni :\n",
    "\n",
    "- `onpromo = 1` si True, sinon 0\n",
    "\n",
    "Si non fourni :\n",
    "\n",
    "- `onpromo = 0` par d√©faut\n",
    "\n",
    "##### C) Encodages statistiques (`store_freq`, `item_freq`)\n",
    "Ces variables ne sont **pas recalcul√©es** sur la nouvelle ligne.\n",
    "\n",
    "Elles utilisent des objets appris au `fit()` :\n",
    "\n",
    "- `store_freq = fr√©quence du store dans train_fit`\n",
    "- `item_freq = fr√©quence du produit dans train_fit`\n",
    "\n",
    "Si le store/item n‚Äôexiste pas dans l‚Äôhistorique :\n",
    "\n",
    "- le `map()` renvoie `NaN`\n",
    "- puis `fillna(0)` ‚Üí on obtient `0`\n",
    "\n",
    "##### D) Variables externes : `transactions_roll14` (avec *feature gap*)\n",
    "Cette feature est obtenue par lookup via :\n",
    "\n",
    "- index `(store_nbr, date)`\n",
    "\n",
    "Deux cas :\n",
    "\n",
    "1. **Date disponible dans `transactions.csv`**  \n",
    "   ‚Üí on r√©cup√®re directement la valeur `transactions_roll14` calcul√©e dans `fit()`\n",
    "\n",
    "2. **Date future ou manquante**  \n",
    "   ‚Üí fallback : la **derni√®re valeur connue** du magasin  \n",
    "   `last_roll_by_store[store]`\n",
    "\n",
    "Si m√™me √ßa n‚Äôexiste pas (store absent) :\n",
    "\n",
    "- fallback final = `0`\n",
    "\n",
    "**Important :** `transactions_roll14` est une variable **au niveau magasin**.  \n",
    "Donc pour un m√™me `(store, date)`, elle est identique pour tous les items du store : c‚Äôest normal.\n",
    "\n",
    "**Feature gap :** si `feature_gap_days = G`, alors :\n",
    "\n",
    "\\[\n",
    "transactions\\_roll14(t)\n",
    "= \\frac{1}{14} \\sum_{j=1}^{14} transactions_{t-(G+j)}\n",
    "\\]\n",
    "\n",
    "Autrement dit, si \\(G=3\\), on utilise \\(t-4..t-17\\) (et pas les 3 jours les plus r√©cents).\n",
    "\n",
    "##### E) Variables externes : `oil_roll14` (sans gap additionnel)\n",
    "M√™me principe :\n",
    "\n",
    "1. si la date existe dans `oil.csv` ‚Üí lookup direct  \n",
    "2. sinon ‚Üí fallback : moyenne des 30 derniers jours connus (calcul√©e au fit)\n",
    "\n",
    "Le test unitaire v√©rifie donc que le pipeline ne casse pas quand la date est future.\n",
    "\n",
    "##### F) Variables jours f√©ri√©s : `is_holiday_effective`\n",
    "Le pipeline v√©rifie si la date est un jour f√©ri√© :\n",
    "\n",
    "- d‚Äôabord au niveau national,\n",
    "- puis overwrite au niveau store (local/r√©gional) si applicable.\n",
    "\n",
    "Si aucune correspondance :\n",
    "\n",
    "- `is_holiday_effective = 0`\n",
    "\n",
    "##### G) Variables de s√©ries temporelles (cible) : lags et rollings (avec *feature gap*)\n",
    "Ces variables doivent absolument respecter l‚Äôanti-leakage et le retard d‚Äôacc√®s aux informations.\n",
    "\n",
    "Dans le pipeline :\n",
    "\n",
    "- `sales_history_days = 120` (fen√™tre max d‚Äôhistorique stock√©e)\n",
    "- `sales_lag28`\n",
    "- `sales_roll14`\n",
    "\n",
    "Le pipeline stocke une s√©rie historique :\n",
    "\n",
    "\\[\n",
    "(store\\_nbr, item\\_nbr, date) \\rightarrow y\n",
    "\\]\n",
    "avec \\(y = unit\\_sales\\_log\\) (ou `unit_sales_clean` selon la cible utilis√©e).\n",
    "\n",
    "**Lag 28 avec gap \\(G\\)** :\n",
    "\\[\n",
    "sales\\_lag28(t) = y_{t-(28+G)}\n",
    "\\]\n",
    "\n",
    "- le pipeline cherche la valeur √† la date `t-(28+G)`\n",
    "- si absent ‚Üí la valeur reste **NaN**\n",
    "- on ajoute un indicateur :\n",
    "  - `sales_lag28_avail = 1` si trouv√©, sinon 0\n",
    "\n",
    "**Rolling 14 avec gap \\(G\\)** :\n",
    "\\[\n",
    "sales\\_roll14(t) = \\frac{1}{cnt}\\sum_{j=1}^{14} y_{t-(G+j)}\n",
    "\\]\n",
    "\n",
    "- le pipeline cherche `y(t-(G+1)) ... y(t-(G+14))`\n",
    "- il additionne ce qui existe (`acc`)\n",
    "- il compte le nombre de valeurs trouv√©es (`cnt`)\n",
    "\n",
    "R√©sultat :\n",
    "\n",
    "- si `cnt > 0` : `sales_roll14 = acc / cnt`\n",
    "- sinon : `sales_roll14 = NaN`\n",
    "\n",
    "On conserve aussi :\n",
    "- `sales_roll14_cnt`\n",
    "- `sales_roll14_frac = cnt/14`\n",
    "\n",
    "‚úÖ **Pourquoi c‚Äôest important ?**  \n",
    "Parce que la strat√©gie `NaN + cnt/frac` permet de distinguer :\n",
    "- ‚Äúhistorique manquant‚Äù ‚Üí NaN (avec `cnt=0`)\n",
    "- ‚Äúventes r√©ellement proches de 0‚Äù ‚Üí valeur proche de 0 (avec `cnt>0`)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### 2.1.6 Exemple de sc√©nario de test unitaire ‚Äúclair‚Äù\n",
    "\n",
    "On choisit une observation :\n",
    "\n",
    "- date = `2017-08-16`\n",
    "- store = `1`\n",
    "- item = `96995`\n",
    "- promo = `False`\n",
    "\n",
    "Le pipeline :\n",
    "\n",
    "1. cr√©e les features date (`dow`, `month`, etc.)\n",
    "2. convertit `promo` en indicateur num√©rique\n",
    "3. r√©cup√®re `store_freq` et `item_freq` appris au fit\n",
    "4. r√©cup√®re `transactions_roll14` (lookup ou derni√®re valeur store)\n",
    "5. r√©cup√®re `oil_roll14` (lookup ou moyenne 30j)\n",
    "6. d√©termine `is_holiday_effective`\n",
    "7. calcule `sales_lag28` et `sales_roll14` avec **feature gap**, √† partir de l‚Äôhistorique uniquement\n",
    "8. renvoie une ligne enrichie exploitable par le mod√®le\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "f87ef570-6e61-494a-83f1-a889c34ae255",
    "_uuid": "8b893b25-ed07-4d59-a3cd-05b05ccd2cf5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-13T14:26:51.955219Z",
     "iopub.status.busy": "2026-01-13T14:26:51.954039Z",
     "iopub.status.idle": "2026-01-13T14:26:52.044638Z",
     "shell.execute_reply": "2026-01-13T14:26:52.043704Z",
     "shell.execute_reply.started": "2026-01-13T14:26:51.955138Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INPUT ===\n",
      "        date  store_nbr  item_nbr  onpromotion\n",
      "0 2017-08-16          2    396540        False\n",
      "\n",
      "=== OUTPUT (features) ===\n",
      "                                        0\n",
      "date                  2017-08-16 00:00:00\n",
      "store_nbr                               2\n",
      "item_nbr                           396540\n",
      "onpromotion                         False\n",
      "year                                 2017\n",
      "month                                   8\n",
      "day                                    16\n",
      "dow                                     2\n",
      "is_weekend                              0\n",
      "onpromo                                 0\n",
      "store_freq                           0.02\n",
      "item_freq                            0.00\n",
      "perishable                              0\n",
      "family_freq_items                    0.00\n",
      "class_freq_items                     0.00\n",
      "class_freq_in_family                 0.00\n",
      "cluster                                13\n",
      "type_freq                            0.36\n",
      "state_freq                           0.41\n",
      "city_freq                            0.40\n",
      "city_freq_in_state                   0.95\n",
      "transactions_roll14               1806.50\n",
      "oil_roll14                          49.06\n",
      "is_holiday_effective                    0\n",
      "sales_lag28                           NaN\n",
      "sales_lag28_avail                       0\n",
      "sales_roll14                          NaN\n",
      "sales_roll14_cnt                        0\n",
      "sales_roll14_frac                    0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Exemple d'input \"prod\"\n",
    "new_row = pd.DataFrame({\n",
    "    \"date\": [\"2017-08-16\"],\n",
    "    \"store_nbr\": [2],\n",
    "    \"item_nbr\": [396540],\n",
    "    \"onpromotion\": [False]\n",
    "})\n",
    "new_row[\"date\"] = pd.to_datetime(new_row[\"date\"])\n",
    "\n",
    "# 2) Transform\n",
    "new_enriched = pipe.transform(new_row)\n",
    "\n",
    "print(\"=== INPUT ===\")\n",
    "print(new_row)\n",
    "\n",
    "print(\"\\n=== OUTPUT (features) ===\")\n",
    "print(new_enriched.T)  # transpose pour voir colonne par colonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mod√©lisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:26:57.145183Z",
     "iopub.status.busy": "2026-01-13T14:26:57.144726Z",
     "iopub.status.idle": "2026-01-13T14:26:59.801498Z",
     "shell.execute_reply": "2026-01-13T14:26:59.800286Z",
     "shell.execute_reply.started": "2026-01-13T14:26:57.145119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from statsmodels.stats.diagnostic import het_white , normal_ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **S√©paration de variable cible et explicatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:27:06.408915Z",
     "iopub.status.busy": "2026-01-13T14:27:06.408393Z",
     "iopub.status.idle": "2026-01-13T14:27:07.140823Z",
     "shell.execute_reply": "2026-01-13T14:27:07.139670Z",
     "shell.execute_reply.started": "2026-01-13T14:27:06.408884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_descript = X_train.loc[:, [\n",
    "    # temporel\n",
    "    \"year\", \"month\", \"day\", \"dow\", \"is_weekend\",\n",
    "\n",
    "    # promotion\n",
    "    \"onpromo\",\n",
    "\n",
    "    # historique des ventes\n",
    "    \"sales_lag28\",\n",
    "    \"sales_lag28_avail\",\n",
    "    \"sales_roll14\",\n",
    "    \"sales_roll14_cnt\",\n",
    "    \"sales_roll14_frac\",\n",
    "\n",
    "    # activit√© magasin\n",
    "    \"transactions_roll14\",\n",
    "\n",
    "    # macro\n",
    "    \"oil_roll14\",\n",
    "\n",
    "    # calendrier\n",
    "    \"is_holiday_effective\",\n",
    "\n",
    "    # structure produit / magasin\n",
    "    \"perishable\",\n",
    "    \"store_freq\",\n",
    "    \"item_freq\",\n",
    "    \"family_freq_items\",\n",
    "    \"class_freq_items\",\n",
    "    \"class_freq_in_family\",\n",
    "    \"cluster\",\n",
    "    \"type_freq\",\n",
    "    \"state_freq\",\n",
    "    \"city_freq\",\n",
    "    \"city_freq_in_state\"\n",
    "]]\n",
    "\n",
    "X_train_cible = X_train[\"unit_sales_log\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:27:12.313475Z",
     "iopub.status.busy": "2026-01-13T14:27:12.313036Z",
     "iopub.status.idle": "2026-01-13T14:27:12.428586Z",
     "shell.execute_reply": "2026-01-13T14:27:12.427283Z",
     "shell.execute_reply.started": "2026-01-13T14:27:12.313431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test_descript = X_test.loc[:, [\n",
    "    # temporel\n",
    "    \"year\", \"month\", \"day\", \"dow\", \"is_weekend\",\n",
    "\n",
    "    # promotion\n",
    "    \"onpromo\",\n",
    "\n",
    "    # historique des ventes\n",
    "    \"sales_lag28\",\n",
    "    \"sales_lag28_avail\",\n",
    "    \"sales_roll14\",\n",
    "    \"sales_roll14_cnt\",\n",
    "    \"sales_roll14_frac\",\n",
    "\n",
    "    # activit√© magasin\n",
    "    \"transactions_roll14\",\n",
    "\n",
    "    # macro\n",
    "    \"oil_roll14\",\n",
    "\n",
    "    # calendrier\n",
    "    \"is_holiday_effective\",\n",
    "\n",
    "    # structure produit / magasin\n",
    "    \"perishable\",\n",
    "    \"store_freq\",\n",
    "    \"item_freq\",\n",
    "    \"family_freq_items\",\n",
    "    \"class_freq_items\",\n",
    "    \"class_freq_in_family\",\n",
    "    \"cluster\",\n",
    "    \"type_freq\",\n",
    "    \"state_freq\",\n",
    "    \"city_freq\",\n",
    "    \"city_freq_in_state\"\n",
    "]]\n",
    "\n",
    "X_test_cible = X_test[\"unit_sales_log\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables explicatives ont √©t√© s√©lectionn√©es de mani√®re √† ne contenir que des informations disponibles au moment de la pr√©diction. Elles regroupent des variables temporelles, des indicateurs de promotion, des statistiques historiques des ventes, des caract√©ristiques structurelles des produits et des magasins, ainsi que des variables macro√©conomiques et calendaires. La variable cible correspond aux ventes unitaires transform√©es par la fonction log(1 + ventes) afin d‚Äôam√©liorer la stabilit√© statistique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standardisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-13T05:43:27.622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standardisons les donn√©es :\n",
    "\n",
    "std_scaler = StandardScaler().fit(X_train_descript)  # Standardize features by removing the mean and scaling to unit variance.\n",
    "X_train_descript_std = std_scaler.transform(X_train_descript)\n",
    "X_test_descript_std = std_scaler.transform(X_test_descript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet de pr√©diction des ventes (`unit_sales`), la base de donn√©es contient de nombreuses variables explicatives sur **des √©chelles tr√®s diff√©rentes** :  \n",
    "- Variables binaires : `onpromotion`, `is_weekend`, `perishable`  \n",
    "- Variables continues : `transactions_roll14`, `oil_roll14`, `sales_lag28`, etc., qui peuvent avoir des valeurs allant de 0 √† plusieurs milliers.  \n",
    "\n",
    "Pour que notre mod√®le ML apprenne de mani√®re stable et efficace, nous avons appliqu√© une **standardisation** des variables continues.  \n",
    "Cette transformation consiste √† centrer chaque variable sur sa moyenne et √† la diviser par son √©cart-type :  \n",
    "\n",
    "$$\n",
    "X_\\text{standardized} = \\frac{X - \\text{moyenne}}{\\text{√©cart-type}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regression lin√©aire**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fonction de metrique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:27:23.844314Z",
     "iopub.status.busy": "2026-01-13T14:27:23.843890Z",
     "iopub.status.idle": "2026-01-13T14:27:23.851228Z",
     "shell.execute_reply": "2026-01-13T14:27:23.849990Z",
     "shell.execute_reply.started": "2026-01-13T14:27:23.844281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Fonction m√©triques\n",
    "# -----------------------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse  = np.mean((y_pred - y_true) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mape = metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    return mse, rmse, mae, 100*mape, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remplacement des NA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:33:43.524406Z",
     "iopub.status.busy": "2026-01-13T14:33:43.523871Z",
     "iopub.status.idle": "2026-01-13T14:33:46.975401Z",
     "shell.execute_reply": "2026-01-13T14:33:46.973853Z",
     "shell.execute_reply.started": "2026-01-13T14:33:43.524368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NA correctement remplac√©s par la valeur suivante non-NA\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Gestion robuste des NA (valeur suivante non-NA)\n",
    "# -----------------------------\n",
    "\n",
    "# 1. Tri temporel (OBLIGATOIRE)\n",
    "X_train_descript = X_train_descript.sort_index()\n",
    "X_test_descript  = X_test_descript.sort_index()\n",
    "\n",
    "# 2. Remplacement par la valeur suivante disponible\n",
    "X_train_descript = X_train_descript.bfill()\n",
    "X_test_descript  = X_test_descript.bfill()\n",
    "\n",
    "# 3. S√©curisation finale (cas extr√™mes uniquement)\n",
    "#    -> s'il reste des NA, on propage la derni√®re valeur connue\n",
    "X_train_descript = X_train_descript.ffill()\n",
    "X_test_descript  = X_test_descript.ffill()\n",
    "\n",
    "# 4. V√©rification stricte\n",
    "assert X_train_descript.isna().sum().sum() == 0, \"NA encore pr√©sents dans X_train\"\n",
    "assert X_test_descript.isna().sum().sum() == 0, \"NA encore pr√©sents dans X_test\"\n",
    "\n",
    "print(\"‚úÖ NA correctement remplac√©s par la valeur suivante non-NA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre jeu de donn√©es, certaines valeurs des variables explicatives sont manquantes (NaN), ce qui emp√™che l‚Äôentra√Ænement de mod√®les de r√©gression comme LinearRegression de scikit-learn, qui ne g√®re pas nativement les valeurs manquantes. Pour r√©soudre ce probl√®me, nous avons choisi d‚Äôimputer les valeurs manquantes en les rempla√ßant par la moyenne (ou alternativement la m√©diane) de chaque variable. Cette m√©thode pr√©sente plusieurs avantages : elle est simple √† mettre en ≈ìuvre, pr√©serve l‚Äôensemble des observations, et r√©duit le risque de biais, surtout lorsque les NaN sont rares ou distribu√©s al√©atoirement. L‚Äôimputation par la moyenne ou la m√©diane permet donc de continuer l‚Äôanalyse tout en conservant la coh√©rence des donn√©es, sans introduire de valeurs extr√™mes qui pourraient affecter la performance du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:33:57.510606Z",
     "iopub.status.busy": "2026-01-13T14:33:57.509459Z",
     "iopub.status.idle": "2026-01-13T14:34:02.635228Z",
     "shell.execute_reply": "2026-01-13T14:34:02.634385Z",
     "shell.execute_reply.started": "2026-01-13T14:33:57.510565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# On cr√©e un mod√®le de r√©gression lin√©aire\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "# On entra√Æne ce mod√®le sur les donn√©es d'entrainement\n",
    "lr.fit(X_train_descript, X_train_cible)\n",
    "\n",
    "# Prediction sur le jeu de donn√©es test comme baseline\n",
    "y_test_pred = lr.predict(X_test_descript)\n",
    "y_train_pred = lr.predict(X_train_descript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calcule des m√©triques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:34:41.687921Z",
     "iopub.status.busy": "2026-01-13T14:34:41.687384Z",
     "iopub.status.idle": "2026-01-13T14:34:42.100278Z",
     "shell.execute_reply": "2026-01-13T14:34:42.098995Z",
     "shell.execute_reply.started": "2026-01-13T14:34:41.687883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_32fb2 caption {\n",
       "  font-size: 16px;\n",
       "  font-weight: bold;\n",
       "  text-align: center;\n",
       "  color: #2a4d69;\n",
       "}\n",
       "#T_32fb2 th {\n",
       "  background-color: green;\n",
       "  color: black;\n",
       "  font-size: 14px;\n",
       "}\n",
       "#T_32fb2_row0_col1, #T_32fb2_row0_col2, #T_32fb2_row0_col3, #T_32fb2_row1_col4 {\n",
       "  background-color: #7f2704;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_32fb2_row0_col4, #T_32fb2_row1_col1, #T_32fb2_row1_col2, #T_32fb2_row1_col3 {\n",
       "  background-color: #fff5eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_32fb2_row0_col5 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_32fb2_row1_col5 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_32fb2\">\n",
       "  <caption>Comparaison des performances du mod√®le de Machine Learning</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_32fb2_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
       "      <th id=\"T_32fb2_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_32fb2_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th id=\"T_32fb2_level0_col3\" class=\"col_heading level0 col3\" >MAE</th>\n",
       "      <th id=\"T_32fb2_level0_col4\" class=\"col_heading level0 col4\" >MAPE (%)</th>\n",
       "      <th id=\"T_32fb2_level0_col5\" class=\"col_heading level0 col5\" >R¬≤</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_32fb2_row0_col0\" class=\"data row0 col0\" >Train</td>\n",
       "      <td id=\"T_32fb2_row0_col1\" class=\"data row0 col1\" >0.710000</td>\n",
       "      <td id=\"T_32fb2_row0_col2\" class=\"data row0 col2\" >0.843000</td>\n",
       "      <td id=\"T_32fb2_row0_col3\" class=\"data row0 col3\" >0.669000</td>\n",
       "      <td id=\"T_32fb2_row0_col4\" class=\"data row0 col4\" >52.436001</td>\n",
       "      <td id=\"T_32fb2_row0_col5\" class=\"data row0 col5\" >0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_32fb2_row1_col0\" class=\"data row1 col0\" >Test</td>\n",
       "      <td id=\"T_32fb2_row1_col1\" class=\"data row1 col1\" >0.693000</td>\n",
       "      <td id=\"T_32fb2_row1_col2\" class=\"data row1 col2\" >0.833000</td>\n",
       "      <td id=\"T_32fb2_row1_col3\" class=\"data row1 col3\" >0.661000</td>\n",
       "      <td id=\"T_32fb2_row1_col4\" class=\"data row1 col4\" >52.558998</td>\n",
       "      <td id=\"T_32fb2_row1_col5\" class=\"data row1 col5\" >0.059000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x79efbd511ee0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Fonction m√©triques\n",
    "# -----------------------------\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse  = np.mean((y_pred - y_true) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    r2   = metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    # MAPE filtr√© (on exclut les z√©ros)\n",
    "    mask = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "    return mse, rmse, mae, 100*mape, r2\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Calcul des m√©triques\n",
    "# -----------------------------\n",
    "train_metrics = compute_metrics(X_train_cible, y_train_pred)\n",
    "test_metrics  = compute_metrics(X_test_cible, y_test_pred)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Tableau comparatif\n",
    "# -----------------------------\n",
    "results = pd.DataFrame({\n",
    "    \"Dataset\": [\"Train\", \"Test\"],\n",
    "    \"MSE\":  [train_metrics[0], test_metrics[0]],\n",
    "    \"RMSE\": [train_metrics[1], test_metrics[1]],\n",
    "    \"MAE\":  [train_metrics[2], test_metrics[2]],\n",
    "    \"MAPE (%)\": [train_metrics[3], test_metrics[3]],\n",
    "    \"R¬≤\":   [train_metrics[4], test_metrics[4]]\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Mise en forme\n",
    "# -----------------------------\n",
    "results_styled = (\n",
    "    results.round(3)\n",
    "           .style\n",
    "           .set_caption(\"Comparaison des performances du mod√®le de Machine Learning\")\n",
    "           .background_gradient(\n",
    "               cmap=\"Oranges\",\n",
    "               subset=[\"MSE\", \"RMSE\", \"MAE\", \"MAPE (%)\"]\n",
    "           )\n",
    "           .background_gradient(\n",
    "               cmap=\"Greens\",\n",
    "               subset=[\"R¬≤\"]\n",
    "           )\n",
    "           .hide(axis=\"index\")\n",
    "           .set_table_styles(\n",
    "               [\n",
    "                   {\"selector\": \"caption\",\n",
    "                    \"props\": [(\"font-size\", \"16px\"),\n",
    "                              (\"font-weight\", \"bold\"),\n",
    "                              (\"text-align\", \"center\"),\n",
    "                              (\"color\", \"#2a4d69\")]},\n",
    "                   {\"selector\": \"th\",\n",
    "                    \"props\": [(\"background-color\", \"green\"),\n",
    "                              (\"color\", \"black\"),\n",
    "                              (\"font-size\", \"14px\")]}\n",
    "               ]\n",
    "           )\n",
    ")\n",
    "\n",
    "results_styled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les r√©sultats de notre mod√®le de r√©gression lin√©aire montrent un MSE de **0.71**, ce qui correspond √† un RMSE de **0.84**, et une MAE de **0.67**. Ces valeurs indiquent que, en moyenne, l‚Äôerreur absolue des pr√©dictions du mod√®le sur le jeu de test est d‚Äôenviron **0.67** unit√©s (en √©chelle log-transform√©e si l‚Äôon utilise unit_sales_log), ce qui est raisonnable pour un premier mod√®le baseline.\n",
    "\n",
    "En ce qui concerne le MAPE, qui mesure l‚Äôerreur en pourcentage, nous avons filtr√© les ventes nulles pour √©viter l‚Äôexplosion de l‚Äôindicateur. Le MAPE filtr√© est de **52.42‚ÄØ%**, ce qui reste √©lev√© mais attendu dans le contexte des ventes retail o√π de nombreux articles ont des ventes tr√®s faibles ou nulles certains jours. Cette valeur souligne que la r√©gression lin√©aire, bien que simple et interpr√©table, ne capture pas encore pleinement la complexit√© temporelle et saisonni√®re des ventes.\n",
    "\n",
    "Le mod√®le pr√©sente un **R¬≤ de 0.064**, ce qui signifie qu‚Äôil n‚Äôexplique que **6,4‚ÄØ%** de la variance des ventes. Autrement dit, la grande majorit√© de la variabilit√© des ventes n‚Äôest pas captur√©e par ce mod√®le simple.\n",
    "\n",
    "En parall√®le, les m√©triques d‚Äôerreur **(RMSE = 0.84, MAE = 0.67, MAPE filtr√© = 52.42‚ÄØ%)** indiquent que, bien que le mod√®le fournisse des pr√©dictions num√©riques, elles restent relativement impr√©cises, notamment pour les articles avec des ventes faibles ou irr√©guli√®res."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modele lin√©aire lasso, rigde, elast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:35:52.911867Z",
     "iopub.status.busy": "2026-01-13T14:35:52.911427Z",
     "iopub.status.idle": "2026-01-13T14:35:52.918610Z",
     "shell.execute_reply": "2026-01-13T14:35:52.917266Z",
     "shell.execute_reply.started": "2026-01-13T14:35:52.911832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Masquer les avertissements\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:35:57.290715Z",
     "iopub.status.busy": "2026-01-13T14:35:57.289008Z",
     "iopub.status.idle": "2026-01-13T14:36:42.582326Z",
     "shell.execute_reply": "2026-01-13T14:36:42.581020Z",
     "shell.execute_reply.started": "2026-01-13T14:35:57.290617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_ridge.py:215: LinAlgWarning: Ill-conditioned matrix (rcond=2.59201e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE Train</th>\n",
       "      <th>RMSE Train</th>\n",
       "      <th>MAE Train</th>\n",
       "      <th>MAPE Train (%)</th>\n",
       "      <th>R¬≤ Train</th>\n",
       "      <th>MSE Test</th>\n",
       "      <th>RMSE Test</th>\n",
       "      <th>MAE Test</th>\n",
       "      <th>MAPE Test (%)</th>\n",
       "      <th>R¬≤ Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge (alpha=1.0)</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.44</td>\n",
       "      <td>34.19</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>34.94</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso (alpha=0.01)</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.44</td>\n",
       "      <td>34.39</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ElasticNet (alpha=0.01, l1_ratio=0.5)</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.44</td>\n",
       "      <td>34.30</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>34.94</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Model  MSE Train  RMSE Train  MAE Train  \\\n",
       "0                      Ridge (alpha=1.0)       0.33        0.57       0.44   \n",
       "1                     Lasso (alpha=0.01)       0.33        0.57       0.44   \n",
       "2  ElasticNet (alpha=0.01, l1_ratio=0.5)       0.33        0.57       0.44   \n",
       "\n",
       "   MAPE Train (%)  R¬≤ Train  MSE Test  RMSE Test  MAE Test  MAPE Test (%)  \\\n",
       "0           34.19      0.57      0.35       0.59      0.45          34.94   \n",
       "1           34.39      0.57      0.35       0.59      0.45          35.06   \n",
       "2           34.30      0.57      0.35       0.59      0.45          34.94   \n",
       "\n",
       "   R¬≤ Test  \n",
       "0     0.52  \n",
       "1     0.52  \n",
       "2     0.52  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Fonction m√©triques\n",
    "# -----------------------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse  = np.mean((y_pred - y_true) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    r2   = metrics.r2_score(y_true, y_pred)\n",
    "    # MAPE filtr√© (on exclut les z√©ros)\n",
    "    mask = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "    return mse, rmse, mae, 100*mape, r2\n",
    "\n",
    "# -----------------------------\n",
    "# 2. D√©finition des mod√®les\n",
    "# -----------------------------\n",
    "models = {\n",
    "    \"Ridge (alpha=1.0)\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso (alpha=0.01)\": Lasso(alpha=0.01, random_state=42, max_iter=10000),\n",
    "    \"ElasticNet (alpha=0.01, l1_ratio=0.5)\": ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42, max_iter=10000)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Boucle sur chaque mod√®le\n",
    "# -----------------------------\n",
    "for name, model in models.items():\n",
    "    # Pipeline : standardisation + mod√®le\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Entra√Ænement\n",
    "    pipe.fit(X_train_descript, X_train_cible)\n",
    "\n",
    "    # Pr√©dictions\n",
    "    y_train_pred = pipe.predict(X_train_descript)\n",
    "    y_test_pred  = pipe.predict(X_test_descript)\n",
    "\n",
    "    # Calcul des m√©triques avec la fonction\n",
    "    metrics_train = compute_metrics(X_train_cible, y_train_pred)\n",
    "    metrics_test  = compute_metrics(X_test_cible, y_test_pred)\n",
    "\n",
    "    # Ajouter au tableau\n",
    "    results.append([name] + list(metrics_train) + list(metrics_test))\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Cr√©ation du DataFrame comparatif\n",
    "# -----------------------------\n",
    "columns = [\"Model\",\n",
    "           \"MSE Train\", \"RMSE Train\", \"MAE Train\", \"MAPE Train (%)\", \"R¬≤ Train\",\n",
    "           \"MSE Test\",  \"RMSE Test\",  \"MAE Test\",  \"MAPE Test (%)\", \"R¬≤ Test\"]\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les r√©sultats montrent que les trois mod√®les lin√©aires p√©nalis√©s (Ridge, Lasso et ElasticNet) offrent des performances quasi identiques, aussi bien sur l‚Äô√©chantillon d‚Äôentra√Ænement que sur l‚Äô√©chantillon de test. Les valeurs de RMSE **(‚âà 0,59 en test), de MAE (‚âà 0,45) et de MAPE (‚âà 35 %)** sont tr√®s proches, tout comme le coefficient de d√©termination R¬≤ d‚Äôenviron 0,52, indiquant une capacit√© explicative moyenne du mod√®le. L‚Äô√©cart tr√®s faible entre les m√©triques train et test sugg√®re l‚Äôabsence de surapprentissage, ce qui confirme une bonne g√©n√©ralisation. Par ailleurs, le fait que **Lasso et ElasticNet** n‚Äôam√©liorent pas significativement les performances par rapport √† Ridge indique que la p√©nalisation L1 (s√©lection de variables) n‚Äôapporte pas de gain notable, ce qui sugg√®re que les variables explicatives sont globalement pertinentes et peu redondantes dans ce cadre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model LightGBM, catboost, Naive, Seasonal Naive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:38:27.872927Z",
     "iopub.status.busy": "2026-01-13T14:38:27.871616Z",
     "iopub.status.idle": "2026-01-13T14:45:32.166809Z",
     "shell.execute_reply": "2026-01-13T14:45:32.165300Z",
     "shell.execute_reply.started": "2026-01-13T14:38:27.872878Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE Train</th>\n",
       "      <th>RMSE Train</th>\n",
       "      <th>MAE Train</th>\n",
       "      <th>MAPE Train (%)</th>\n",
       "      <th>R¬≤ Train</th>\n",
       "      <th>MSE Test</th>\n",
       "      <th>RMSE Test</th>\n",
       "      <th>MAE Test</th>\n",
       "      <th>MAPE Test (%)</th>\n",
       "      <th>R¬≤ Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.41</td>\n",
       "      <td>32.01</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.43</td>\n",
       "      <td>32.61</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.41</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.43</td>\n",
       "      <td>32.93</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>80.97</td>\n",
       "      <td>-0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Seasonal Naive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.43</td>\n",
       "      <td>113.48</td>\n",
       "      <td>-5.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  MSE Train  RMSE Train  MAE Train  MAPE Train (%)  R¬≤ Train  \\\n",
       "0        LightGBM       0.28        0.53       0.41           32.01      0.63   \n",
       "1        CatBoost       0.29        0.53       0.41           32.39      0.62   \n",
       "2           Naive        NaN         NaN        NaN             NaN       NaN   \n",
       "3  Seasonal Naive        NaN         NaN        NaN             NaN       NaN   \n",
       "\n",
       "   MSE Test  RMSE Test  MAE Test  MAPE Test (%)  R¬≤ Test  \n",
       "0      0.31       0.56      0.43          32.61     0.58  \n",
       "1      0.31       0.56      0.43          32.93     0.58  \n",
       "2      1.01       1.00      0.86          80.97    -0.37  \n",
       "3      4.83       2.20      1.43         113.48    -5.55  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Packages\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Fonction m√©triques\n",
    "# -----------------------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    mse  = np.mean((y_pred - y_true) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    r2   = metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    mask = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "    return mse, rmse, mae, 100*mape, r2\n",
    "\n",
    "# -----------------------------\n",
    "# 2. BASELINES CORRECTES\n",
    "# -----------------------------\n",
    "# Naive : derni√®re valeur connue\n",
    "y_naive_pred_test = np.full(\n",
    "    shape=len(X_test_cible),\n",
    "    fill_value=X_train_cible.values[-1]\n",
    ")\n",
    "\n",
    "# Seasonal Naive (p√©riode = 7)\n",
    "season = 7\n",
    "last_season = X_train_cible.values[-season:]\n",
    "y_seasonal_naive_test = np.tile(\n",
    "    last_season,\n",
    "    int(np.ceil(len(X_test_cible) / season))\n",
    ")[:len(X_test_cible)]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Mod√®les ML (PARAM√àTRES STABLES)\n",
    "# -----------------------------\n",
    "models = {\n",
    "    \"LightGBM\": lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        random_state=42,\n",
    "        verbosity=-1,\n",
    "        force_row_wise=True\n",
    "    ),\n",
    "    \"CatBoost\": CatBoostRegressor(\n",
    "        iterations=300,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Entra√Ænement & √©valuation ML\n",
    "# -----------------------------\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_descript, X_train_cible)\n",
    "\n",
    "    y_train_pred = model.predict(X_train_descript)\n",
    "    y_test_pred  = model.predict(X_test_descript)\n",
    "\n",
    "    results.append(\n",
    "        [name]\n",
    "        + list(compute_metrics(X_train_cible, y_train_pred))\n",
    "        + list(compute_metrics(X_test_cible, y_test_pred))\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Ajout des baselines (TEST ONLY)\n",
    "# -----------------------------\n",
    "results.append(\n",
    "    [\"Naive\"]\n",
    "    + [np.nan]*5\n",
    "    + list(compute_metrics(X_test_cible, y_naive_pred_test))\n",
    ")\n",
    "\n",
    "results.append(\n",
    "    [\"Seasonal Naive\"]\n",
    "    + [np.nan]*5\n",
    "    + list(compute_metrics(X_test_cible, y_seasonal_naive_test))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Tableau final\n",
    "# -----------------------------\n",
    "columns = [\n",
    "    \"Model\",\n",
    "    \"MSE Train\", \"RMSE Train\", \"MAE Train\", \"MAPE Train (%)\", \"R¬≤ Train\",\n",
    "    \"MSE Test\",  \"RMSE Test\",  \"MAE Test\",  \"MAPE Test (%)\",  \"R¬≤ Test\"\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "results_df = results_df.sort_values(\"R¬≤ Test\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optimisation des hyperparam√®tres**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validation crois√©e temporelle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:40:20.130634Z",
     "iopub.status.busy": "2026-01-13T13:40:20.130286Z",
     "iopub.status.idle": "2026-01-13T13:40:20.135938Z",
     "shell.execute_reply": "2026-01-13T13:40:20.134812Z",
     "shell.execute_reply.started": "2026-01-13T13:40:20.130605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grilles de param√®tres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:40:29.651596Z",
     "iopub.status.busy": "2026-01-13T13:40:29.651291Z",
     "iopub.status.idle": "2026-01-13T13:40:29.657677Z",
     "shell.execute_reply": "2026-01-13T13:40:29.656743Z",
     "shell.execute_reply.started": "2026-01-13T13:40:29.651572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"Ridge\": {\n",
    "        \"model__alpha\": np.logspace(-3, 2, 50)\n",
    "    },\n",
    "    \"Lasso\": {\n",
    "        \"model__alpha\": np.logspace(-4, 1, 50)\n",
    "    },\n",
    "    \"ElasticNet\": {\n",
    "        \"model__alpha\": np.logspace(-4, 1, 30),\n",
    "        \"model__l1_ratio\": np.linspace(0.1, 0.9, 9)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D√©finition des pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:40:36.209541Z",
     "iopub.status.busy": "2026-01-13T13:40:36.209194Z",
     "iopub.status.idle": "2026-01-13T13:40:36.215565Z",
     "shell.execute_reply": "2026-01-13T13:40:36.214384Z",
     "shell.execute_reply.started": "2026-01-13T13:40:36.209511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Ridge optimis√©\": Ridge(random_state=42),\n",
    "    \"Lasso optimis√©\": Lasso(random_state=42, max_iter=20000),\n",
    "    \"ElasticNet optimis√©\": ElasticNet(random_state=42, max_iter=20000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimisation des hyperparam√®tres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-13T05:43:27.625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=param_grids[name.split()[0]],\n",
    "        n_iter=20,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        cv=tscv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    search.fit(X_train_descript, X_train_cible)\n",
    "\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"Meilleurs param√®tres :\", search.best_params_)\n",
    "\n",
    "    # Pr√©dictions\n",
    "    y_train_pred = best_model.predict(X_train_descript)\n",
    "    y_test_pred  = best_model.predict(X_test_descript)\n",
    "\n",
    "    # M√©triques\n",
    "    train_metrics = compute_metrics(X_train_cible, y_train_pred)\n",
    "    test_metrics  = compute_metrics(X_test_cible, y_test_pred)\n",
    "\n",
    "    results.append([name] + list(train_metrics) + list(test_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tableau comparatif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-13T05:43:27.625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns = [\"Model\",\n",
    "           \"MSE Train\", \"RMSE Train\", \"MAE Train\", \"MAPE Train (%)\", \"R¬≤ Train\",\n",
    "           \"MSE Test\",  \"RMSE Test\",  \"MAE Test\",  \"MAPE Test (%)\", \"R¬≤ Test\"]\n",
    "\n",
    "results_df_optimized_linear = pd.DataFrame(results, columns=columns)\n",
    "results_df_optimized_linear = results_df_optimized_linear.sort_values(\n",
    "    \"R¬≤ Test\", ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "results_df_optimized_linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L‚Äôoptimisation des hyperparam√®tres des mod√®les Ridge, Lasso et ElasticNet √† l‚Äôaide de RandomizedSearchCV et d‚Äôune validation crois√©e temporelle a permis d‚Äôam√©liorer leur capacit√© de g√©n√©ralisation. En ajustant le param√®tre de r√©gularisation Œ± (et le ratio L1/L2 pour ElasticNet), les mod√®les optimis√©s pr√©sentent une r√©duction des erreurs de pr√©diction ainsi qu‚Äôune am√©lioration du coefficient de d√©termination R¬≤ sur le jeu de test. Ces r√©sultats confirment l‚Äôimportance de la r√©gularisation adapt√©e dans les mod√®les lin√©aires, en particulier dans un contexte de donn√©es corr√©l√©es et de grande dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimisation de lightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:41:43.328629Z",
     "iopub.status.busy": "2026-01-13T13:41:43.328276Z",
     "iopub.status.idle": "2026-01-13T14:00:26.896090Z",
     "shell.execute_reply": "2026-01-13T14:00:26.895179Z",
     "shell.execute_reply.started": "2026-01-13T13:41:43.328601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "\n",
      "==============================\n",
      "Meilleurs param√®tres LightGBM\n",
      "==============================\n",
      "{'subsample': 1.0, 'num_leaves': 50, 'n_estimators': 300, 'min_child_samples': 50, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "Meilleur RMSE CV : 0.5160968116783878\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Mod√®le LightGBM (silencieux et stable)\n",
    "# -----------------------------\n",
    "lgb_model = LGBMRegressor(\n",
    "    random_state=42,\n",
    "    verbosity=-1,        # supprime les logs\n",
    "    force_row_wise=True  # √©vite les warnings + plus stable\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Grille de param√®tres R√âDUITE (cl√©)\n",
    "# -----------------------------\n",
    "param_grid_lgb = {\n",
    "    \"n_estimators\": [200, 300],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"num_leaves\": [31, 50],\n",
    "    \"max_depth\": [-1, 10],\n",
    "    \"min_child_samples\": [50, 100],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Validation temporelle\n",
    "# -----------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Randomized Search S√âCURIS√â\n",
    "# -----------------------------\n",
    "lgb_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_grid_lgb,\n",
    "    n_iter=5,                 # üî• cl√© : r√©duit mais suffisant\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=tscv,\n",
    "    random_state=42,\n",
    "    n_jobs=1,                 # üî• √©vite crash m√©moire\n",
    "    verbose=1                 # üî• feedback visible\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Entra√Ænement\n",
    "# -----------------------------\n",
    "lgb_search.fit(X_train_descript, X_train_cible)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. AFFICHAGE GARANTI\n",
    "# -----------------------------\n",
    "print(\"\\n==============================\")\n",
    "print(\"Meilleurs param√®tres LightGBM\")\n",
    "print(\"==============================\")\n",
    "print(lgb_search.best_params_)\n",
    "\n",
    "print(\"\\nMeilleur RMSE CV :\",\n",
    "      -lgb_search.best_score_)\n",
    "\n",
    "best_lgb = lgb_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimisation de Catboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-13T05:43:27.625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Mod√®le CatBoost\n",
    "# -----------------------------\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    loss_function=\"RMSE\",\n",
    "    iterations=800,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    random_seed=42,\n",
    "    verbose=200,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Grille de param√®tres\n",
    "# -----------------------------\n",
    "param_grid_cat = {\n",
    "    \"iterations\": [300, 500, 800],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"depth\": [4, 6, 8],\n",
    "    \"l2_leaf_reg\": [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Random Search\n",
    "# -----------------------------\n",
    "cat_search = RandomizedSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_distributions=param_grid_cat,\n",
    "    n_iter=15,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=tscv,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cat_model.fit(\n",
    "    X_train_descript,\n",
    "    X_train_cible,\n",
    "    eval_set=(X_test_descript, X_test_cible),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "\n",
    "best_cat = cat_search.best_estimator_\n",
    "print(\"Meilleurs param√®tres CatBoost :\", cat_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparaison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-13T05:43:27.626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Pr√©dictions\n",
    "# -----------------------------\n",
    "y_train_pred_lgb = best_lgb.predict(X_train_descript)\n",
    "y_test_pred_lgb  = best_lgb.predict(X_test_descript)\n",
    "\n",
    "y_train_pred_cat = best_cat.predict(X_train_descript)\n",
    "y_test_pred_cat  = best_cat.predict(X_test_descript)\n",
    "\n",
    "# -----------------------------\n",
    "# Calcul des m√©triques\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "for name, ytr, yte in [\n",
    "    (\"LightGBM optimis√©\", y_train_pred_lgb, y_test_pred_lgb),\n",
    "    (\"CatBoost optimis√©\", y_train_pred_cat, y_test_pred_cat)\n",
    "]:\n",
    "    train_metrics = compute_metrics(X_train_cible, ytr)\n",
    "    test_metrics  = compute_metrics(X_test_cible, yte)\n",
    "\n",
    "    results.append([name] + list(train_metrics) + list(test_metrics))\n",
    "\n",
    "# -----------------------------\n",
    "# Tableau comparatif\n",
    "# -----------------------------\n",
    "columns = [\"Model\",\n",
    "           \"MSE Train\", \"RMSE Train\", \"MAE Train\", \"MAPE Train (%)\", \"R¬≤ Train\",\n",
    "           \"MSE Test\",  \"RMSE Test\",  \"MAE Test\",  \"MAPE Test (%)\", \"R¬≤ Test\"]\n",
    "\n",
    "results_df_optimized = pd.DataFrame(results, columns=columns)\n",
    "results_df_optimized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L‚Äôoptimisation des hyperparam√®tres via RandomizedSearchCV a permis d‚Äôam√©liorer les performances des mod√®les LightGBM et CatBoost. Compar√©s aux versions non optimis√©es, les mod√®les ajust√©s pr√©sentent une r√©duction des erreurs de pr√©diction (RMSE et MAPE) ainsi qu‚Äôune am√©lioration du coefficient de d√©termination R¬≤ sur le jeu de test. Ces r√©sultats confirment l‚Äôimportance du tuning des hyperparam√®tres pour exploiter pleinement la capacit√© pr√©dictive des mod√®les de type Gradient Boosting, en particulier dans un contexte de s√©ries temporelles avec de nombreuses variables explicatives."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 44328,
     "sourceId": 7391,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
